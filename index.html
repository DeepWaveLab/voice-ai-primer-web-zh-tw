<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>語音 AI 與語音助理 | 圖解入門指南</title>
  <meta name="description" content="2025年語音AI的全面指南">
  <meta property="og:title" content="語音 AI 與語音助理 | 圖解入門指南">
  <meta property="og:description" content="2025年語音AI的全面指南">
  <meta property="og:image" content="images/meta.jpg">
  <meta property="og:type" content="website">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="語音 AI 與語音助理 | 圖解入門指南">
  <meta name="twitter:description" content="2025年語音AI的全面指南">
  <meta name="twitter:image" content="images/meta.jpg">
  <link rel="stylesheet" href="styles.css">
  <link rel="icon" href="images/favicon.ico">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,wght@0,400;0,500;0,700;1,400&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=DM+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://use.typekit.net/rff5lbb.css">
  <link rel="stylesheet" href="script/binary-numbers.css">
  <style>
    #table-of-contents ul {
      list-style-type: none;
      padding-left: 20px;
    }
  </style>
</head>
<body>
  <div class="container">
    <div style="float: right; margin: 1rem 0 1rem 1rem; width: 10rem;">
      <img src="images/Figure 0100 horizontal.svg">
    </div>


    <header class="header">
      <h1 class="title">語音 AI 與語音助理</h1>
      <h2 class="subtitle">圖解入門指南</h2>
    </header>
    

    <nav id="table-of-contents">
      <h2 class="table-of-contents-title">目錄</h2>
      <ol>
        <li><a href="#conversational-voice-ai">2025年的對話式語音 AI</a></li>
        <li><a href="#about-this-guide">關於本指南</a></li>
        <li><a href="#basic-loop">基本對話式 AI 循環</a></li>
        <li>
          <a href="#core-tech">核心技術與最佳實踐</a>
          <ul>
            <li><a href="#latency">4.1. 延遲</a></li>
            <li>
              <a href="#llms-for-voice">4.2. 語音用例的 LLMs</a>
              <ul>
                <li><a href="#latency-llm">4.2.1. 延遲</a></li>
                <li><a href="#cost-comparison">4.2.2. 成本比較</a></li>
                <li><a href="#open-source">4.2.3. 開源/開放權重</a></li>
                <li><a href="#speech-to-speech">4.2.4. 語音到語音模型呢？</a></li>
              </ul>
            </li>
            <li>
              <a href="#speech-to-text">4.3. 語音轉文字</a>
              <ul>
                <li><a href="#deepgram-and-gladia">4.3.1. Deepgram 和 Gladia</a></li>
                <li><a href="#prompting-help">4.3.2. 提示可以幫助 LLM</a></li>
                <li><a href="#other-stt-options">4.3.3. 其他語音轉文字選項</a></li>
                <li><a href="#gemini-transcribing">4.3.4. 使用 Google Gemini 轉錄</a></li>
              </ul>
            </li>
            <li>
              <a href="#text-to-speech">4.4. 文字轉語音</a>
            </li>
            <li>
              <a href="#audio-processing">4.5. 音訊處理</a>
              <ul>
                <li><a href="#avoiding-spurious-interruptions">4.5.1. 麥克風和自動增益控制</a></li>
                <li><a href="#echo-cancellation">4.5.2. 回音消除</a></li>
                <li><a href="#noise-suppression">4.5.3. 噪音抑制、語音和音樂</a></li>
                <li><a href="#encoding">4.5.4. 編碼</a></li>
                <li><a href="#server-side-noise">4.5.5. 伺服器端噪音處理和說話者隔離</a></li>
                <li><a href="#voice-activity-detection">4.5.6. 語音活動檢測</a></li>
              </ul>
            </li>
            <li>
              <a href="#network-transport">4.6. 網路傳輸</a>
              <ul>
                <li><a href="#websockets-webrtc">4.6.1. WebSockets 和 WebRTC</a></li>
                <li><a href="#http">4.6.2. HTTP</a></li>
                <li><a href="#quic-moq">4.6.3. QUIC 和 MoQ</a></li>
                <li><a href="#network-routing">4.6.4. 網路路由</a></li>
              </ul>
            </li>
            <li>
              <a href="#turn-detection">4.7. 輪次檢測</a>
              <ul>
                <li><a href="#voice-activity-detection-4-7">4.7.1 語音活動檢測</a></li>
                <li><a href="#push-to-talk">4.7.2. 按鍵通話</a></li>
                <li><a href="#endpoint-markers">4.7.3. 端點標記</a></li>
                <li><a href="#context-aware-turn-detection">4.7.4. 上下文感知輪次檢測</a></li>
              </ul>
            </li>
            <li>
              <a href="#interruption-handling">4.8. 中斷處理</a>
              <ul>
                <li><a href="#avoiding-spurious-interruptions">4.8.1. 避免虛假中斷</a></li>
                <li><a href="#maintaining-accurate-context">4.8.2. 中斷後維持準確的上下文</a></li>
              </ul>
            </li>
            <li>
              <a href="#managing-conversation-context">4.9. 管理對話上下文</a>
              <ul>
                <li><a href="#differences-between-llm-apis">4.9.1. LLM API 之間的差異</a></li>
                <li><a href="#modifying-context-between-turns">4.9.2. 輪次之間修改上下文</a></li>
              </ul>
            </li>
            <li>
              <a href="#function-calling">4.10. 函數呼叫</a>
              <ul>
                <li><a href="#function-calling-reliability">4.10.1. 語音 AI 上下文中的函數呼叫可靠性</a></li>
                <li><a href="#latency-function-calls">4.10.2. 函數呼叫延遲</a></li>
                <li><a href="#handling-interruptions">4.10.3. 處理中斷</a></li>
                <li><a href="#streaming-mode">4.10.4. 串流模式和函數呼叫區塊</a></li>
                <li><a href="#execute-function-calls">4.10.5. 如何以及在哪裡執行函數呼叫</a></li>
                <li><a href="#async-function-calls">4.10.6. 非同步函數呼叫</a></li>
                <li><a href="#parallel-composite-function-calling">4.10.7. 並行和複合函數呼叫</a></li>
              </ul>
            </li>
            <li><a href="#multimodality">4.11. 多模態</a></li>
          </ul>
        </li>
        <li>
          <a href="#multiple-models">使用多個 AI 模型</a>
          <ul>
            <li><a href="#fine-tuned-models">5.1. 使用多個微調模型</a></li>
            <li><a href="#async-inference-tasks">5.2. 執行非同步推論任務</a></li>
            <li><a href="#content-guardrails">5.3. 內容護欄</a></li>
            <li><a href="#single-inference-actions">5.4. 執行單一推論動作</a></li>
            <li><a href="#self-improving-systems">5.5. 邁向自我改進系統</a></li>
          </ul>
        </li>
        <li>
          <a href="#scripting">腳本編寫和指令遵循</a>
        </li>
        <li>
          <a href="#evals">語音 AI 評估</a>
          <ul>
            <li><a href="#evals-different">7.1. 語音 AI 評估與軟體單元測試的不同</a></li>
            <li><a href="#failure-modes">7.2. 失敗模式</a></li>
            <li><a href="#eval-strategy">7.3. 制定評估策略</a></li>
          </ul>
        </li>
        <li><a href="#telephony">與電話系統整合</a></li>
        <li>
          <a href="#rag-memory">RAG 和記憶</a>
        </li>
        <li>
          <a href="#hosting">託管和擴展</a>
          <ul>
            <li><a href="#hosting-architecture">10.1. 架構</a></li>
            <li><a href="#hosting-cost">10.2. 計算每分鐘成本</a></li>
          </ul>
        </li>
        <li>
          <a href="#future">2025年的展望</a>
        </li> 
        <li><a href="#contributors">貢獻者</a></li>
      </ol>
    </nav>

    <main>
      <div class="chunk-row">
        <div class="chunk-content">

          <h1 id="conversational-voice-ai">1. 2025年的對話式語音 AI</h1>
            
            <p>大型語言模型（LLMs）是優秀的對話夥伴。</p>
            
            <p>如果你曾經花時間與 ChatGPT 或 Claude 進行自由形式的對話，你會直覺地感受到與 LLM 交談感覺相當自然且廣泛實用。</p>
            
            <p>LLMs 也擅長將非結構化資訊轉換為結構化資料。<sup>[1]</sup></p>
            
            <p>新型語音 AI 代理利用這兩種 LLM 能力 — 對話，以及從非結構化資料中提取結構 — 創造出一種全新的使用者體驗。</p>
          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[1] 這裡我們是廣義地表達，而非指某些 LLM 的「結構化輸出」功能的狹義含義。</p>
              </div>
            </div>
          </div>
        </div>

        <div class="chunk-row">
          <div class="chunk-content">
            <p>語音 AI 目前已在各種商業環境中被部署。例如：</p>
            
            <ul class="arrow-list">
              <li>在醫療預約前收集病患資料，</li>
              <li>跟進潛在銷售線索，</li>
              <li>處理越來越多樣化的客服中心任務，</li>
              <li>協調公司間的排程和物流，以及</li>
              <li>為幾乎所有類型的小型企業接聽電話。</li>
            </ul>
            
            <p>在消費者方面，對話式語音（和視訊）AI 也開始進入社交應用和遊戲領域。開發者每天都在 GitHub 和社群媒體上分享個人語音 AI 專案和實驗。</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="about-this-guide">2. 關於本指南</h1>

          <p>本指南是語音 AI 最新技術的快照。<sup>[2]</sup></p>
                        
            <p>建立生產級語音代理相當複雜。許多元素從零開始實作並非易事。如果你開發語音 AI 應用，你可能會依賴框架來處理本文件中討論的許多事項。但我們認為了解這些組件如何配合是有用的，無論你是從頭開始建構所有內容，還是不是。</p>
            
            <p>本指南直接受到 Sean DuBois 的開源書籍 <a href="https://webrtcforthecurious.com" target="_blank">WebRTC For the Curious</a> 的啟發。自四年前首次發布以來，該書已幫助無數開發者快速掌握 WebRTC。<sup>[3]</sup></p>
            
            <p>本文件中的語音 AI 程式碼範例使用 <a href="https://pipecat.ai" target="_blank">Pipecat</a> 開源框架。Pipecat 是一個供應商中立的即時 AI 代理層。<sup>[4]</sup> 我們在本文件中使用 Pipecat 是因為：</p>
            
            <ol class="list-decimal">
              <li>我們每天都用它來開發並協助維護它，所以我們對它很熟悉！</li>
              <li>Pipecat 目前是使用最廣泛的語音 AI 框架，NVIDIA、Google、AWS、OpenAI 和數百家新創公司的團隊都在利用並貢獻程式碼。</li>
            </ol>
            
            <p>我們在本文件中嘗試提供一般性建議，而非推薦商業產品和服務。當我們強調特定供應商時，是因為它們被大比例的語音 AI 開發者使用。</p>

            
            <p>讓我們開始吧……</p>

        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[2] 我們最初為 2025 年 2 月的 AI 工程峰會撰寫本指南。我們在 2025 年 5 月中旬更新了它。</p>
            </div>
            <div class="footnote">
              <p>[3] <a href="https://webrtcforthecurious.com" target="_blank">webrtcforthecurious.com</a> — WebRTC 與語音 AI 相關，我們將在後面的 <a href="#websockets-webrtc">WebSockets 和 WebRTC</a> 部分討論。</p>
            </div>
            <div class="footnote">
              <p>[4] Pipecat 整合了超過 60 個 <a href="https://docs.pipecat.ai/server/services/supported-services" target="_blank">AI 模型和服務</a>，並具有最先進的對話輪替偵測和中斷處理實作。你可以使用 Pipecat 編寫使用 WebSockets、WebRTC、HTTP 和電話系統與使用者通訊的程式碼。Pipecat 包含適用於各種基礎設施平台的傳輸實作，包括 Twilio、Telnyx、LiveKit、Daily 等。有適用於 JavaScript、React、iOS、Android 和 C++ 的 <a href="https://docs.pipecat.ai/client/introduction" target="_blank">客戶端 Pipecat SDK</a>。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <h1 id="basic-loop">3. 基本對話式 AI 循環</h1>

            <p>語音 AI 代理的基本「工作」是聆聽人類所說的內容，以某種有用的方式回應，然後重複這個序列。</p>
            
            <p>現今生產環境中的語音代理幾乎都有非常相似的架構。語音代理程式在雲端運行並協調語音對語音循環。代理程式使用多個 AI 模型，有些在代理本地運行，有些通過 API 訪問。代理程式還使用 LLM 函數呼叫或結構化輸出與後端系統整合。</p>

            
            <ol class="list-decimal">
              <li>語音由使用者裝置上的麥克風捕獲，編碼後通過網路發送到雲端運行的語音代理程式。</li>
              <li>輸入語音被轉錄，為 LLM 創建文字輸入。</li>
              <li>文字被組合成上下文 — 提示 — 並由 LLM 執行推論。推論輸出通常會被代理程式邏輯過濾或轉換。<sup>[5]</sup></li>
              <li>輸出文字被發送到文字轉語音模型以創建音訊輸出。</li>
              <li>音訊輸出被發送回使用者。</li>
            </ol>
            
            <p>你會注意到語音代理程式在雲端運行，且文字轉語音、LLM 和語音轉文字處理都在雲端進行。長期來看，我們預期會看到更多 AI 工作負載在裝置上運行。然而，目前<strong>生產環境的語音 AI 非常以雲端為中心</strong>，原因有二：</p>
            
            <ol class="list-decimal">
              <li>語音 AI 代理需要使用最佳的 AI 模型，以可靠地執行複雜工作流程並保持低延遲。終端使用者裝置尚未擁有足夠的 AI 計算能力，無法以可接受的延遲運行最佳的 STT、LLM 和 TTS 模型。</li>
              <li>目前大多數商業語音 AI 代理是通過電話與使用者通訊。對於電話通話，沒有終端使用者裝置 — 至少，沒有一個你可以在上面運行任何程式碼的裝置！</li>
            </ol>
            
            <p>讓我們深入<sup>[6]</sup>這個代理協調世界，並回答以下問題：</p>
            
            <ol class="list-decimal">
              <li>哪些 LLM 最適合語音 AI 代理？</li>
              <li>如何在長時間運行的會話中管理對話上下文？</li>
              <li>如何將語音代理連接到現有的後端系統？<sup>[7]</sup></li>
              <li>如何知道你的語音代理表現良好？</li>
            </ol>

        </div>
        
        <div class="chunk-notes">
          <div class="chapter-image">
            <img src="images/Figure 0200.svg" class="image-hide-narrow" width="100%">
            <p class="image-caption image-hide-narrow">目前幾乎所有生產環境語音 AI 代理的架構</p>
          </div>
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[5] 例如，檢測常見的 LLM 錯誤和安全問題。</p>
            </div>
            <div class="footnote">
              <p>[6] 讓我們深入探討 — 編輯</p>
            </div>
            <div class="footnote">
              <p>[7] 例如，CRM、專有知識庫和客服中心系統。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
           <h1 id="core-tech">4. 核心技術與最佳實踐</h1>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">  
            <h2 id="latency">4.1. 延遲</h2>
            
            <p>建立語音代理在大多數方面與其他類型的 AI 工程相似。如果你有建立基於文字的多輪 AI 代理的經驗，你在該領域的許多經驗也將在語音方面派上用場。</p>
            
            <p><strong>最大的差異在於延遲。</strong></p>
            
            <p>人類在正常對話中期望快速回應。500毫秒的回應時間是典型的。長時間的停頓感覺不自然。</p>
            
            <p>如果你正在建立語音 AI 代理，值得學習如何從終端使用者的角度準確測量延遲。</p>
            
            <p>你經常會看到 AI 平台引用的延遲並非真正的「語音到語音」測量。這通常不是惡意的。從提供者的角度來看，測量延遲的簡單方法是測量推論時間。所以這就是提供者習慣思考延遲的方式。然而，這種伺服器端的視角並未考慮音訊處理、短語結束點延遲、網路傳輸和作業系統開銷。</p>
            
            <p><strong>手動測量語音到語音延遲很容易。</strong></p>
            
            <p>只需錄製對話，將錄音載入音訊編輯器，查看音訊波形，並測量從使用者語音結束到 LLM 語音開始的時間。</p>
            
            <p>如果你為生產環境建立對話式語音應用程式，偶爾以這種方式檢查你的延遲數據是值得的。如果在進行這些測試時添加模擬網路封包丟失和抖動，那就更好了！</p>
            
            <p>以程式方式測量真正的語音到語音延遲是具有挑戰性的。一些延遲發生在作業系統深處。因此，大多數可觀察性工具只測量到第一個（音訊）位元組的時間。這是總語音到語音延遲的合理代理，但請再次注意，你沒有測量的事項 — 如短語結束點變化和網路往返時間 — 如果你沒有方法追蹤它們，可能會成為問題。</p>
            
            <p><strong>如果你正在建立對話式 AI 應用程式，800毫秒的語音到語音延遲是一個很好的目標。</strong>以下是從使用者麥克風到雲端再返回的語音到語音往返的細分。這些數字相當典型，總計約為1秒。在當今的 LLM 中，持續達到800毫秒是具有挑戰性的，但並非不可能！</p>

            <table class="data-table latency-breakdown">
              <thead>
                <tr>
                  <th>階段</th>
                  <th>時間 (毫秒)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>macOS 麥克風輸入</td>
                  <td>40</td>
                </tr>
                <tr>
                  <td>opus 編碼</td>
                  <td>21</td>
                </tr>
                <tr class="network-row">
                  <td>網路堆疊和傳輸</td>
                  <td>10</td>
                </tr>
                <tr>
                  <td>封包處理</td>
                  <td>2</td>
                </tr>
                <tr>
                  <td>抖動緩衝區</td>
                  <td>40</td>
                </tr>
                <tr>
                  <td>opus 解碼</td>
                  <td>1</td>
                </tr>
                <tr>
                  <td>轉錄和結束點檢測</td>
                  <td>300</td>
                </tr>
                <tr>
                  <td>LLM 首字節時間</td>
                  <td>350</td>
                </tr>
                <tr>
                  <td>句子聚合</td>
                  <td>20</td>
                </tr>
                <tr>
                  <td>TTS 首字節時間</td>
                  <td>120</td>
                </tr>
                <tr>
                  <td>opus 編碼</td>
                  <td>21</td>
                </tr>
                <tr>
                  <td>封包處理</td>
                  <td>2</td>
                </tr>
                <tr class="network-row">
                  <td>網路堆疊和傳輸</td>
                  <td>10</td>
                </tr>
                <tr>
                  <td>抖動緩衝區</td>
                  <td>40</td>
                </tr>
                <tr>
                  <td>opus 解碼</td>
                  <td>1</td>
                </tr>
                <tr>
                  <td>macOS 喇叭輸出</td>
                  <td>15</td>
                </tr>
                <tr>
                  <td>總毫秒數</td>
                  <td>993</td>
                </tr>
              </tbody>
            </table>
            
            <p class="table-caption">語音到語音對話往返 — 延遲細分。</p>

            <p>我們已經展示了 Pipecat 代理可以通過在同一個 GPU 啟用的叢集中託管所有模型，並優化所有模型以降低延遲而非提高吞吐量，從而實現 500 毫秒的語音到語音延遲。這種方法目前並不廣泛使用。託管模型成本高昂。而且開放權重 LLM 在語音 AI 中的使用頻率低於最佳專有模型，如 GPT-4o 或 Gemini。有關語音代理 LLM 的討論，請參閱下一節。</p>

            <p>由於延遲對語音用例非常重要，因此在本指南中將經常提到延遲。</p>
        </div>
        <div class="chunk-notes">
          <div class="image-hide-narrow">
            <img src="images/Figure 0300.svg" width="100%">
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="llms-for-voice">4.2. 語音用例的 LLMs</h2>
            
            <p>2023年3月GPT-4的發布開啟了當前語音AI的新時代。GPT-4是第一個既能維持靈活的多輪對話，又能被精確提示以執行有用工作的LLM。如今，GPT-4的繼任者 – GPT-4o – 仍然是對話式語音AI的主導模型。</p>
            
            <p>現在有幾個模型在語音AI關鍵領域的表現已經與原始GPT-4一樣好或更好：</p>
            
            <ul class="arrow-list">
              <li>足夠低的延遲以進行互動式語音對話。</li>
              <li>良好的指令遵循能力。<sup>[8]</sup></li>
              <li>可靠的函數呼叫。<sup>[9]</sup></li>
              <li>較低的幻覺率和其他不適當回應。</li>
              <li>個性和語調。</li>
              <li>成本。</li>
            </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[8] 提示模型執行特定任務有多容易？</p>
            </div>
            <div class="footnote">
              <p>[9] 語音AI代理heavily依賴函數呼叫。</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>但今天的GPT-4o也比原始GPT-4更好！特別是在指令遵循、函數呼叫和降低幻覺率方面。</p>

            <p>GPT-4o的主要競爭對手是Google的Gemini 2.0 Flash。Gemini 2.0 Flash速度快，在指令遵循和函數呼叫方面與GPT-4o不相上下，且價格極具競爭力。</p>

            <p><strong>語音AI用例的要求相當高，通常使用最佳可用模型是合理的。</strong>在某個時間點，這種情況會改變，非最先進的模型也將足以在語音AI用例中廣泛採用。但目前還不是這樣。</p>
            

        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="latency-llm">4.2.1 LLM 延遲</h3>
            
            <p>Claude Sonnet本應是語音AI的絕佳選擇，但推理延遲（首個token的時間）並非Anthropic的優先事項。Claude Sonnet的中位數延遲通常是GPT-4o和Gemini Flash的兩倍，且P95分布範圍更大。</p>
            
            <table class="data-table model-comparison">
              <thead>
                <tr>
                  <th>模型</th>
                  <th>中位數 TTFT (毫秒)</th>
                  <th>P95 TTFT (毫秒)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>GPT-4o</td>
                  <td>460</td>
                  <td>580</td>
                </tr>
                <tr>
                  <td>GPT-4o mini</td>
                  <td>290</td>
                  <td>420</td>
                </tr>
                <tr>
                  <td>GPT-4.1</td>
                  <td>450</td>
                  <td>670</td>
                </tr>
                <tr>
                  <td>Gemini 2.0 Flash</td>
                  <td>380</td>
                  <td>450</td>
                </tr>
                <tr>
                  <td>Llama 4 Maverick (Groq)</td>
                  <td>290</td>
                  <td>360</td>
                </tr>
                <tr>
                  <td>Claude Sonnet 3.7</td>
                  <td>1,410</td>
                  <td>2,140</td>
                </tr>
              </tbody>
            </table>
            <p class="table-caption">OpenAI、Anthropic和Google API的首個token時間(TTFT)指標 - 2025年5月</p>
            
            <p>一個粗略的經驗法則：LLM首個token時間在500毫秒或更低對大多數語音AI用例來說已經足夠好。GPT-4o的TTFT通常在400-500毫秒。Gemini Flash也類似。</p>
            
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="cost-comparison">4.2.2 成本比較</h3>
            
            <p>推理成本一直在定期且迅速下降。因此，一般來說，LLM成本在選擇使用哪個LLM時是最不重要的因素。Gemini 2.0 Flash最新公布的定價比GPT-4o便宜10倍。我們將看到這對語音AI領域有何影響。</p>
            
            <table class="data-table model-comparison">
              <thead>
                <tr>
                  <th>模型</th>
                  <th>3分鐘對話</th>
                  <th>10分鐘對話</th>
                  <th>30分鐘對話</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>GPT-4o</td>
                  <td>$0.009</td>
                  <td>$0.08</td>
                  <td>$0.75</td>
                </tr>
                <tr>
                  <td>Gemini 2.0 Flash</td>
                  <td>$0.0004</td>
                  <td>$0.004</td>
                  <td>$0.03</td>
                </tr>
              </tbody>
            </table>
            <p class="table-caption">多輪對話的會話成本隨著時間增長呈超線性增長。30分鐘的會話成本大約是3分鐘會話的100倍。您可以通過快取、上下文摘要和其他技術來降低長會話的成本。</p>
            
            <p>請注意，成本隨著會話長度的增加呈超線性增長。除非您在會話期間修剪或摘要上下文，否則長會話的成本會成為問題。這對於語音到語音模型尤其如此（見<a href="#speech-to-speech">下文</a>）。</p>
            
            <p>上下文增長的數學特性使得確定語音對話的每分鐘成本變得棘手。此外，API提供商越來越多地提供token快取功能，這可以抵消成本（並減少延遲），但增加了估算不同用例成本的複雜性。</p>
            
            <p>OpenAI的<a href="https://community.openai.com/t/new-realtime-api-voices-and-cache-pricing/998238" target="_blank">OpenAI Realtime API自動token快取</a>特別好用。Google最近為所有2.5版本模型推出了類似功能，稱為<a href="https://ai.google.dev/gemini-api/docs/caching?lang=python" target="_blank">隱式快取</a>。</p>
            

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="open-source">4.2.3 開源 / 開放權重模型</h3>
            
            <p>Meta的Llama 3.3和4.0開放權重模型在基準測試中表現優於原始的GPT-4。目前，它們在商業用例中通常不如GPT-4o和Gemini。然而，能夠基於這些模型進行開發並在自己的基礎設施上運行它們具有重要意義。<sup>[11]</sup></p>
            
            <p>許多提供商提供Llama推理端點，而無伺服器GPU平台為部署自己的Llama提供了多種選擇。Meta最近宣布了新的第一方推理API，並強烈表示開放權重的Llama模型是公司的關鍵戰略重點。</p>

            <p>Llama家族中一個有趣且功能強大的模型是Ultravox。Ultravox是一個<a href="https://github.com/fixie-ai/ultravox" target="_blank">開源的原生音訊LLM</a>。<a href="https://ultravox.ai/" target="_blank">Ultravox背後的公司</a>還提供企業級的託管語音到語音API。Ultravox利用多種技術將Llama 3.3擴展到音訊領域，並改進基礎模型在語音AI用例中的指令遵循和函數調用性能。Ultravox是開源AI生態系統優勢以及原生音訊模型引人注目潛力的典範。</p>
            
            <p>我們在2025年看到開源/開放權重模型取得了很大進展。Llama 4剛剛推出，社群仍在評估其在多輪對話AI用例中的實際表現。來自阿里巴巴的新Qwen 3模型是優秀的中型模型，在早期基準測試中與Llama 4平分秋色。此外，DeepSeek、Google (Gemma)和Microsoft (Phi)未來的開放權重模型很可能成為語音AI用例的良好選擇。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[11] 如果您計劃為您的用例微調LLM，Llama 3.3 70B是一個很好的起點。下文將詳細介紹微調。</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="speech-to-speech">4.2.4 語音到語音模型呢？</h3>
            
            <p>語音到語音模型是一個令人興奮的相對較新的發展。語音到語音LLM可以接受音訊而非文字作為輸入，並能直接產生音訊輸出。這消除了語音代理協調循環中的語音轉文字和文字轉語音部分。</p>
            
            <p>語音到語音模型的潛在優勢是：</p>
            
            <ul class="arrow-list">
              <li>更低的延遲。</li>
              <li>更好地理解人類對話的細微差別。</li>
              <li>更自然的語音輸出。</li>
            </ul>
            
            <p>OpenAI和Google都發布了語音到語音API。大多數訓練大型模型和構建語音AI應用的人都認為語音到語音模型是語音AI的未來。</p>
            
            <p>然而，目前的語音到語音模型和API還不足以應對大多數生產環境中的語音AI用例。</p>
            
            <p>當今最好的語音到語音模型確實比當今最好的文字到語音模型聽起來更自然。OpenAI的<a href="https://platform.openai.com/docs/guides/audio" target="_blank">gpt4o-audio-preview</a> <sup>[12]</sup>模型確實聽起來像是語音AI未來的預覽。</p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[12] <a href="https://platform.openai.com/docs/guides/audio" target="_blank">OpenAI音訊API文檔</a></p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <p>不過，語音到語音模型還沒有文字模式LLM那麼成熟和可靠。</p>
          
          <ul class="arrow-list">
              <li>理論上可以實現更低的延遲，但音訊使用的token比文字多。較大的token上下文處理起來對LLM來說更慢。在實際應用中，目前音訊模型在長時間多輪對話中通常比文字模型慢。<sup>[13]</sup></li>
              <li>更好的理解似乎確實是這些模型的真正優勢。這在Gemini 2.0 Flash音訊輸入中特別明顯。對於gpt-4o-audio-preview，情況目前不太明確，它是一個比文字模式GPT-4o更小且功能稍弱的模型。</li>
              <li>更自然的語音輸出目前確實可以明顯感知到。但音訊LLM在音訊模式下確實有一些奇怪的輸出模式，這些在文字模式下不太常見：詞語重複、有時落入恐怖谷的話語標記，以及偶爾無法完成句子。</li>
          </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[13] 音訊模型的這種延遲問題顯然可以通過快取、巧妙的API設計以及模型本身的架構演進來解決。</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
          <p>這些問題中最大的是多輪音訊所需的更大上下文大小。解決這個問題並獲得原生音訊優勢而不受上下文大小缺點影響的一種方法是將每個對話輪次處理為文字和音訊的混合。使用音訊處理最近的用戶訊息；使用文字處理其餘的對話歷史。</p>

          <p>OpenAI的測試版語音到語音產品 — OpenAI Realtime API — 速度快且語音質量驚人。但該API背後的模型是較小的gpt-4o-audio-preview，而非完整的GPT-4o。因此，指令遵循和函數調用不如完整版好。使用Realtime API管理對話上下文也很棘手，且API有一些新產品的粗糙邊緣。<sup>[14]</sup></p>
            
          <p>Google Multimodal Live API是另一個有前途的 — 且處於演進早期的 — 語音到語音服務。這個API提供了Gemini模型近期未來的展望：長上下文窗口、出色的視覺能力、快速推理、強大的音訊理解、代碼執行和搜索基礎。與OpenAI Realtime API一樣，Multimodal Live API尚未成為大多數生產語音AI應用的正確選擇。</p>

          <p>請注意，語音到語音API相對昂貴。我們為<a href="https://dub.sh/voice-agents-010" target="_blank">OpenAI Realtime API構建了一個計算器</a>，顯示成本如何隨會話長度擴展，同時考慮了OpenAI非常好的自動token快取功能。</p>
            
          <p>我們預計2025年語音到語音領域將取得大量進展。但生產環境中的語音AI應用從多模型方法轉向使用語音到語音API的速度仍是一個開放性問題。</p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[14] 參見<a href="https://latent.space/p/realtime-api" target="_blank">關於Realtime API的詳細說明</a></p>
            </div>
          </div>
          <div class="footnote">
            <a href="https://dub.sh/voice-agents-010" target="_blank"><img src="images/Figure 0700 Spreadsheet.png" width="90%"></a>
            <p class="image-caption">OpenAI Realtime API成本計算器</p>
          </div>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="speech-to-text">4.3. 語音轉文字</h2>
            
          <p>語音轉文字是語音AI的「輸入」階段。語音轉文字也常被稱為<i>轉錄</i>或<i>ASR</i>（自動語音識別）。</p>
            
          <p>對於語音AI用例，我們需要非常低的轉錄延遲和非常低的詞錯誤率。可惜的是，優化語音模型以降低延遲會對準確性產生負面影響。</p>
            
          <p>目前有幾個非常好的轉錄模型，但它們<em>不是</em>為低延遲而設計的。Whisper是一個在許多產品和服務中使用的開源模型。它非常好，但通常首個token的生成時間在500毫秒或更長，因此很少用於對話式語音AI用例。</p>         
        </div>
        <div class="chunk-notes">
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="deepgram-and-gladia">4.3.1 Deepgram和Gladia</h3>
            
            <p>目前大多數生產環境中的語音AI代理使用<a href="https://deepgram.com" target="_blank">Deepgram</a>或<a href="https://gladia.io" target="_blank">Gladia</a>進行語音轉文字。Deepgram是一家商業語音轉文字AI實驗室和API平台，長期以來一直提供低延遲、低詞錯誤率和低成本的絕佳組合。Gladia是該領域的較新參與者（成立於2022年），在多語言支持方面具有特殊優勢。</p>
            
            <p>Deepgram的模型可作為自助服務API使用，也可作為Docker容器供客戶在自己的系統上運行。大多數人開始時通過API使用Deepgram語音轉文字。對於美國用戶，首個token的生成時間通常為150毫秒。</p>
            
            <p>管理可擴展的GPU集群是一項重要的持續性運維工作，因此從API轉向在自己的基礎設施上託管模型不應該沒有充分理由就進行。充分的理由包括：</p>
            
            <ul class="arrow-list">
              <li>保持音訊/轉錄數據的私密性。Deepgram提供BAA和數據處理協議，但一些客戶可能希望完全控制音訊和轉錄數據。美國以外的客戶可能有法律義務將數據保留在自己的國家或地區內。（請注意，根據默認的Deepgram服務條款，他們可以使用您通過API發送給他們的所有數據進行訓練。企業計劃可以選擇退出此項。）</li>
              <li>減少延遲。Deepgram在美國以外沒有推理服務器。從歐洲使用Deepgram，首個token生成時間約為250毫秒；從印度使用則約為350毫秒。</li>
            </ul>
            
            <p>Deepgram提供微調服務，如果您的用例包含相對不常見的詞彙、語音風格或口音，這可以幫助降低詞錯誤率。</p>

            <p>Gladia是我們在英語世界以外的新語音AI項目中最常見的語音轉文字提供商。Gladia總部位於法國，在美國和歐洲都有推理服務器，並支持100多種語言。</p>

            <p>Gladia提供託管API和在自己的基礎設施上運行其模型的選項。Gladia的API可用於需要歐洲數據駐留的應用程序。</p>

        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="prompting-help">4.3.2 提示可以幫助LLM</h3>
            
            <p>很大比例的轉錄錯誤源於實時流中轉錄模型可用的上下文非常有限。</p>
            
            <p>當今的LLM足夠聰明，可以解決轉錄錯誤。當LLM執行推理時，它可以訪問完整的對話上下文。因此，您可以告訴LLM輸入是用戶語音的轉錄，它應該相應地進行推理。</p>

            <pre><code>You are a helpful, concise, and reliable voice assistant. Your primary goal is to understand the user's spoken requests, even if the speech-to-text transcription contains errors. Your responses will be converted to speech using a text-to-speech system. Therefore, your output must be plain, unformatted text.

When you receive a transcribed user request:
1. Silently correct for likely transcription errors. Focus on the intended meaning, not the literal text. If a word sounds like another word in the given context, infer and correct. For example, if the transcription says "buy milk two tomorrow" interpret this as "buy milk tomorrow".
2. Provide short, direct answers unless the user explicitly asks for a more detailed response. For example, if the user says "what time is it?" you should respond with "It is 2:38 AM". If the user asks "Tell me a joke", you should provide a short joke.
3. Always prioritize clarity and accuracy. Respond in plain text, without any formatting, bullet points, or extra conversational filler.
4. If you are asked a question that is time dependent, use the current date, which is February 3, 2025, to provide the most up to date information.
5. If you do not understand the user request, respond with "I'm sorry, I didn't understand that."

Your output will be directly converted to speech, so your response should be natural-sounding and appropriate for a spoken conversation.
</code></pre>
            <p class="image-caption">語音AI代理的示例提示語言。</p>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="other-stt-options">4.3.3 其他語音轉文字選項</h3>

            <p>我們預計2025年語音轉文字領域將出現許多新發展。截至2025年4月初，我們正在追蹤的一些新發展：</p>

            <ul class="arrow-list">
              <li>OpenAI <a href="https://openai.com/index/introducing-our-next-generation-audio-models/">剛剛發布</a>了兩個新的語音轉文字模型，gpt-4o-transcribe和gpt-4o-mini-transcribe。</li>
              <li>另外兩家備受推崇的語音技術公司，<a href="https://speechmatics.com/">Speechmatics</a>和<a href="https://assembly.ai/">AssemblyAI</a>，已開始更多關注對話式語音用例，推出流式API和具有更快首個token生成時間的模型。</li>
              <li>NVIDIA正在發布<a href="https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/">開源語音模型</a>，在基準測試中表現極為出色。</li>
              <li>推理公司<a href="https://groq.com/">Groq</a>託管的Whisper Large v3 Turbo版本現在的中位數首個token生成時間低於300毫秒，使其成為對話式語音應用的可選項。這是我們看到的第一個達到這種延遲的Whisper API服務。</li>
            </ul>
            
            <p>所有大型雲服務都有語音轉文字API。目前，對於低延遲語音AI用例，它們都不如Deepgram或Gladia好。</p>
            
            <p>但在以下情況下，您可能想使用Azure AI Speech、Amazon Transcribe或Google Speech-to-Text：</p>
            
            <ul class="arrow-list">
              <li>您已經與這些雲提供商之一有大量承諾支出或數據處理安排。</li>
              <li>您有很多這些雲提供商的創業積分可以使用！</li>
            </ul>
            
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="gemini-transcribing">4.3.4 使用Google Gemini進行轉錄</h3>
            
            <p>利用Gemini 2.0 Flash作為低成本、原生音訊模型優勢的一種方法是同時使用Gemini 2.0進行對話生成和轉錄。</p>
            
            <p>為此，我們需要運行兩個並行的推理過程。</p>

            <ul class="arrow-list">
              <li>一個推理過程生成對話回應。</li>
              <li>另一個推理過程轉錄用戶的語音。</li>
              <li>每個音訊輸入僅用於一個輪次。完整的對話上下文始終是最近用戶語音的音訊，加上所有先前輸入和輸出的文字轉錄。</li>
              <li>這給您提供了兩全其美的優勢：對當前用戶話語的原生音訊理解；整個上下文的token數量減少。<sup>[15]</sup></li>
            </ul>
          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[15] 用文字替換音訊可將token數量減少約10倍。對於十分鐘的對話，這將處理的總token數量 - 因此輸入token的成本 - 減少約100倍。（因為對話歷史在每個輪次都會累積。）</p>
              </div>
            </div>
          </div>
        </div>

        <div class="chunk-row">
          <div class="chunk-content">

            <p>以下是將這些並行推理過程實現為Pipecat管道的代碼。</p>

            <pre class="language-python"><code>
  pipeline = Pipeline( 
    [   
        transport.input(), 
        audio_collector,
        context_aggregator.user(),
        ParallelPipeline( 
            [ # transcribe
                input_transcription_context_filter,
                input_transcription_llm,
                transcription_frames_emitter,
            ],
            [ # conversation inference
                conversation_llm,
            ],
        ),
        tts,
        transport.output(),
        context_aggregator.assistant(),
        context_text_audio_fixup, 
    ] 
  )            
            </code></pre>

            <p>邏輯如下。</p>
            
            <ol class="list-decimal">
              <li>對話LLM接收文字形式的對話歷史，加上每個新輪次的用戶語音作為原生音訊，並輸出對話回應。</li>
              <li>輸入轉錄LLM接收相同的輸入，但輸出最近用戶語音的字面轉錄。</li>
              <li>在每個對話輪次結束時，用戶音訊上下文條目被替換為該音訊的轉錄。</li>
            </ol>

            <p>Gemini的每token成本如此之低，以至於這種方法實際上比使用Deepgram進行轉錄更便宜。</p>

            <p>重要的是要理解，我們在這裡不是將Gemini 2.0 Flash用作完整的語音到語音模型，但我們<em>確實</em>在使用其原生音訊理解能力。我們提示模型使其以兩種不同的「模式」運行，對話和轉錄。</p>

            <p>以這種方式使用LLM展示了SOTA LLM架構和功能的強大。這種方法新到仍處於實驗階段，但早期測試表明，它可以產生比任何其他當前技術更好的對話理解和更準確的轉錄。然而，也有缺點。轉錄延遲不如使用專門的語音轉文字模型好。運行兩個推理過程和交換上下文元素的複雜性很大。通用LLM容易受到提示注入和上下文遵循錯誤的影響，而專門的轉錄模型不會受到這些影響。</p>

            <p>以下是轉錄的系統指令（提示）。</p>

            <pre><code>
You are an audio transcriber. You are receiving audio from a user. Your job is to transcribe the input audio to text exactly as it was said by the user.

You will receive the full conversation history before the audio input, to help with context. Use the full history only to help improve the accuracy of your transcription.

Rules:
- Respond with an exact transcription of the audio input.
- Do not include any text other than the transcription.
- Do not explain or add to your response.
- Transcribe the audio input simply and precisely.
- If the audio is not clear, emit the special string "".
- No response other than exact transcription, or "", is allowed.
            </code></pre>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="text-to-speech">4.4. 文字轉語音</h2>
            
            <p>文字轉語音是語音到語音處理循環的輸出階段。</p>
            
            <p>語音AI開發者根據以下因素選擇語音模型/服務：</p>
            
            <ul class="arrow-list">
              <li>語音聽起來有多自然（整體質量）<sup>[16]</sup></li>
              <li>延遲<sup>[17]</sup></li>
              <li>成本</li>
              <li>語言支持</li>
              <li>詞級時間戳支持</li>
              <li>自定義語音、口音和發音的能力</li>
            </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[16] 發音、語調、節奏、重音、韻律、情感表達。</p>
            </div>
            <div class="footnote">
              <p>[17] 首個音訊位元組的輸出時間。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>2024年語音選項顯著擴展。新創公司紛紛出現。最佳語音品質大幅提升。且每個供應商都改善了延遲問題。</p>
            
            <p>與語音轉文字一樣，所有大型雲端供應商都有文字轉語音產品。<sup>[18]</sup> 但大多數語音AI開發者並不使用它們，因為目前新創公司的模型表現更好。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[18] Azure AI Speech、Amazon Polly和Google Cloud Text-to-Speech。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content"> 
            
            <p>在即時對話語音模型領域中最具影響力的實驗室有（按字母順序）：</p>
            
            <ul class="arrow-list">
              <li>Cartesia – 使用創新的狀態空間模型架構，同時實現高品質和低延遲。</li>
              <li>Deepgram – 優先考慮延遲和低成本。</li>
              <li>ElevenLabs – 強調情感和上下文真實感。</li>
              <li>Rime – 提供可自定義的TTS模型，專門針對對話語音訓練。</li>
            </ul>
            
            <p>這四家公司都擁有強大的模型、工程團隊以及穩定高效的API。Cartesia、Deepgram和Rime的模型可以部署在您自己的基礎設施上。</p>

            <table class="data-table voice-model-breakdown">
              <thead>
                <tr>
                  <th> </th>
                  <th>每分鐘成本（約）</th>
                  <th>中位數TTFB (毫秒)</th>
                  <th>P95 TTFB (毫秒)</th>
                  <th>平均前置靜音時間 (毫秒)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Cartesia</td>
                  <td>$0.02</td>
                  <td>190</td>
                  <td>260</td>
                  <td>160</td>
                </tr>
                <tr>
                  <td>Deepgram</td>
                  <td>$0.008</td>
                  <td>150</td>
                  <td>320</td>
                  <td>260</td>
                </tr>
                <tr>
                  <td>ElevenLabs Turbo v2</td>
                  <td>$0.08</td>
                  <td>300</td>
                  <td>510</td>
                  <td>160</td>
                </tr>
                <tr>
                  <td>ElevenLabs Flash v2</td>
                  <td>$0.04</td>
                  <td>170</td>
                  <td>190</td>
                  <td>100</td>
                </tr>
                <tr>
                  <td>Rime</td>
                  <td>$0.024</td>
                  <td>340</td>
                  <td>980</td>
                  <td>160</td>
                </tr>
              </tbody>
            </table>
            <p class="table-caption">大規模使用時的每分鐘約略成本及首位元組時間指標 – 2025年2月。請注意，成本取決於承諾使用量和所用功能。平均前置靜音毫秒是指在第一個語音幀之前，音訊流中的平均初始靜音間隔。</p>

            <p>與語音轉文字一樣，非英語語音模型的品質和支援差異很大。如果您正在為非英語使用場景構建語音AI，您可能需要進行更廣泛的測試 — 測試更多服務和更多語音，以找到令您滿意的解決方案。</p>
            
            <p>所有語音模型有時會發音錯誤，且不一定知道如何發音專有名詞或不常見的詞彙。</p>
            
            <p>某些服務提供調整發音的功能。如果您預先知道文字輸出將包含特定專有名詞，這會很有幫助。如果您的語音服務不支援語音調整，您可以提示LLM輸出特定單詞的「聽起來像」的拼寫。例如，用in-vidia代替NVIDIA。</p>

            <pre ><code class="nobreak">
              Replace "NVIDIA" with "in vidia" and replace <br/>
              "GPU" with "gee pee you" in your responses.
            </code></pre>
            <p class="image-caption">通過LLM文字輸出調整發音的提示語言示例</p>
            



            <p>對於對話語音使用場景，能夠追蹤用戶聽到的文字對於維持準確的對話上下文很重要。這要求模型除了音訊外還能生成詞級時間戳元數據，並且時間戳數據可以重建回原始輸入文字。這是語音模型的相對較新功能。上表中除了ElevenLabs Flash外的所有模型都支援詞級時間戳。</p>

            <pre><code>
{
  "type": "timestamps",
  "context_id": "test-01",
  "status_code": 206,
  "done": false,
  "word_timestamps": {
    "words": ["What's", "the", "capital", "of", "France?"],
    "start": [0.02, 0.3, 0.48, 0.6, 0.8],
    "end": [0.3, 0.36, 0.6, 0.8, 1]
  }
}
                    </code></pre>

                    <p class="image-caption">來自Cartesia API的詞級時間戳。</p>

            <p>此外，一個真正穩固的即時串流API非常有幫助。對話語音應用程式經常並行觸發多個音訊推理。語音代理程式碼需要能夠中斷進行中的推理，並將每個推理請求與輸出流關聯起來。來自語音模型提供商的串流API都相對較新且仍在發展中。目前，Cartesia和Rime在Pipecat中擁有最成熟的串流支援。</p>

            <p>我們預計語音模型在2025年將繼續進步。</p>
            
            <ul class="arrow-list">
              <li>上述幾家公司中的幾家已暗示將在上半年推出新模型。</li>
              <li>OpenAI <a href="https://openai.com/index/introducing-our-next-generation-audio-models/" target="_blank">最近發布了</a>一個新的文字轉語音模型，gpt-4o-mini-tts。這個模型完全可調整，為告訴語音模型不僅是<em>說什麼</em>，還有<em>如何說</em>開啟了新的可能性。您可以在<a href="https://openai.fm" target="_blank">openai.fm</a>上嘗試調整gpt-4o-mini-tts。</li>
              <li><a href="https://groq.com/" target="_blank">Groq</a>和<a href="https://play.ai/" target="_blank">PlayAI</a>最近<a href="https://groq.com/build-fast-with-text-to-speech/" target="_blank">宣布合作</a>。Groq以快速推理聞名，而PlayAI提供支援30多種語言的低延遲語音模型。</li>
            </ul>
        </div>
        
        <div class="chunk-notes">


          
          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="audio-processing">4.5. 音訊處理</h2>
            
            <p>一個好的語音AI平台或函式庫大多會隱藏音訊捕獲和處理的複雜性。但如果您構建複雜的語音代理，在某些時候您會遇到音訊處理中的錯誤和邊緣情況。<sup>[19]</sup> 因此，值得快速瀏覽一下音訊輸入管道。</p>
        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[19] …這適用於軟體中的所有事物，也許也適用於生活中的大多數事物。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="avoiding-spurious-interruptions">4.5.1 麥克風和自動增益控制</h3>
            
            <p>現今的麥克風是極其複雜的硬體設備，與大量低階軟體相結合。這通常很好 — 我們可以從內置於移動設備、筆記型電腦和藍牙耳機中的微型麥克風獲得出色的音訊。</p>
            
            <p>但有時這些低階軟體不會按我們的期望運作。特別是藍牙設備可能會為語音輸入增加數百毫秒的延遲。作為語音AI開發者，這在很大程度上超出了您的控制範圍。但值得注意的是，延遲可能會因特定用戶使用的操作系統和輸入設備而有很大差異。</p>

            <div class="chunk-image-inline">
              <img src="images/Figure 1600.jpg" alt="藍牙有問題？一直都有。" class="microphone-image" width="98%">
            </div>
            <p>大多數音訊捕獲管道會對輸入信號應用一定程度的自動增益控制。這通常是您想要的，因為它可以補償諸如用戶與麥克風的距離等因素。您通常可以禁用部分自動增益控制，但在消費級設備上，您通常無法完全禁用它。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="echo-cancellation">4.5.2 回音消除</h3>
            
            <p>如果用戶將手機貼在耳邊，或戴著耳機，您不需要擔心本地麥克風和揚聲器之間的反饋。但如果用戶使用免提電話，或使用沒有耳機的筆記型電腦，那麼良好的回音消除就極為重要。</p>
            
            <p>回音消除對延遲非常敏感，因此回音消除必須在設備上運行（而不是在雲端）。如今，出色的回音消除功能已內置於電話系統、網頁瀏覽器和WebRTC原生移動SDK中。<sup>[20]</sup></p>
            
            <p>因此，如果您使用語音AI、WebRTC或電話SDK，您應該有可以在幾乎所有實際場景中「正常運作」的回音消除功能。如果您自行開發語音AI捕獲管道，您將需要弄清楚如何整合回音消除邏輯。例如，如果您正在構建基於WebSocket的React Native應用程式，默認情況下您將沒有任何回音消除功能。<sup>[21]</sup></p>
        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[20] 請注意，Firefox的回音消除效果不是很好。我們建議語音AI開發者以Chrome和Safari作為主要平台進行構建，並且只在時間允許的情況下將Firefox作為次要平台進行測試。</p>
            </div>
            <div class="footnote">
              <p>[21] 我們最近幫助某人調試他們的React Native應用程式的音訊問題。根本原因是他們沒有意識到需要實現回音消除，因為他們沒有使用語音AI或WebRTC SDK。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="noise-suppression">4.5.3 噪音抑制、語音和音樂</h3>
            
            <p>用於電話和WebRTC的音訊捕獲管道幾乎總是默認為「語音模式」。語音可以比音樂壓縮得多，而且噪音減少和回音消除算法對於較窄頻帶信號更容易實現。</p>
            
            <p>許多電話平台僅支援8khz音訊。按現代標準來看，這種品質明顯較低。如果您通過具有此限制的系統路由，您對此無能為力。您的用戶可能會也可能不會注意到品質差異 — 大多數人對電話通話音訊的期望較低。</p>
            
            <p>WebRTC支援非常高品質的音訊。<sup>[22]</sup> WebRTC的默認設置通常是48khz採樣率、單聲道、32 kbs Opus編碼，以及中等噪音抑制算法。這些設置針對語音進行了優化。它們適用於各種設備和環境，通常是語音AI的正確選擇。</p>
            
            <p>音樂在這些設置下聽起來不會好！</p>
            
            <p>如果您需要通過WebRTC連接傳送音樂，您需要：</p>
            
            <ul class="arrow-list">
              <li>關閉回音消除（用戶需要戴耳機）。</li>
              <li>關閉噪音抑制。</li>
              <li>可選擇啟用立體聲。</li>
              <li>增加Opus編碼比特率（單聲道64 kbs是個不錯的目標，立體聲則為96 kbs或128 kbs）。</li>
            </ul>
        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[22] 高品質音訊的一些使用場景：</p>
              <ul class="arrow-list">
                <li>與LLM教師的音樂課。</li>
                <li>錄製包含背景聲音或音樂的播客。</li>
                <li>互動式生成AI音樂。</li>
              </ul>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="encoding">4.5.4 編碼</h3>
            
            <p>編碼是指音訊數據如何格式化以通過網絡連接傳送的通用術語。<sup>[23]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[23] （或用於保存在文件中。）</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class=""chunk-content">
            
            <p>實時通信常用的編碼包括：</p>
            
            <ul class="arrow-list">
              <li>16位PCM格式的未壓縮音訊。</li>
              <li>Opus — WebRTC和某些電話系統。</li>
              <li>G.711 — 一種廣泛支援的標準電話編解碼器。</li>
            </ul>

            <table class="data-table">
              <tr>
                <th>編解碼器</th>
                <th>比特率</th>
                <th>品質</th>
                <th>使用場景</th>
              </tr>
              <tr>
                <td>16-bit PCM</td>
                <td>384 kbps (單聲道 24 kHz)</td>
                <td>非常高（接近無損）</td>
                <td>語音錄製、嵌入式系統、簡單解碼至關重要的環境</td>
              </tr>
              <tr>
                <td>Opus 32 kbps</td>
                <td>32 kbps</td>
                <td>良好（針對語音優化的心理聲學壓縮）</td>
                <td>視訊通話、低頻寬串流、播客</td>
              </tr>
              <tr>
                <td>Opus 96 kbps</td>
                <td>96 kbps</td>
                <td>非常好到優秀（心理聲學壓縮）</td>
                <td>串流、音樂、音訊存檔</td>
              </tr>
              <tr>
                <td>G.711 (8 kHz)</td>
                <td>64 kbps</td>
                <td>差（有限頻寬，以語音為中心）</td>
                <td>傳統VoIP系統、電話、傳真傳輸、語音訊息</td>
              </tr>
            </table>
            <p class="image-caption">語音AI最常用的音訊編解碼器</p>
            
            <p>Opus是這三個選項中最好的。Opus內置於網頁瀏覽器中，從頭開始設計為低延遲編解碼器，且非常高效。它在各種比特率下表現良好，並支援語音和高保真使用場景。</p>
            
            <p>16位PCM是「原始音訊」。您可以直接將PCM音訊幀發送到軟體聲道（假設正確指定了採樣率和數據類型）。但請注意，這種未壓縮的音訊通常不是您想通過互聯網連接發送的內容。24khz PCM的比特率為384 kbs。這是一個足夠大的比特率，以至於許多來自終端用戶設備的實際連接將難以實時傳送這些位元組。</p>
        </div>
        
        <div class="chunk-notes">
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="server-side-noise">4.5.5 伺服器端噪音處理和說話者隔離</h3>
            
            <p>語音轉文字和語音活動檢測模型通常可以忽略一般環境噪音 – 街道聲音、狗吠聲、靠近麥克風的嘈雜風扇、鍵盤點擊聲。因此，對於許多人與人之間的使用場景至關重要的傳統「噪音抑制」算法對於語音AI並不那麼重要。</p>
            
            <p>但有一種音訊處理對語音AI特別有價值：主要說話者隔離。主要說話者隔離可抑制背景語音。這可以顯著提高轉錄準確性。</p>
            
            <p>想像一下嘗試在機場等環境中與語音代理交談。您的手機麥克風可能會捕捉到大量來自登機口廣播和路過人群的背景語音。您不希望LLM看到的文字轉錄中包含那些背景語音！</p>
            
            <p>或者想像一下在客廳中有電視或收音機在背景中播放的用戶。由於人類通常相當擅長過濾低音量的背景語音，人們不一定會想到在撥打客戶支援電話前關閉電視或收音機。</p>
            
            <p>您可以在自己的語音AI管道中使用的最佳可用說話者隔離模型由<a href="https://krisp.ai" target="_blank">Krisp</a>提供。許可證針對企業用戶，價格不便宜。但對於大規模商業用途，語音代理性能的提升足以證明成本的合理性。</p>

            <p>OpenAI最近在其Realtime API中推出了新的噪音減少功能。參考文檔在<a href="https://platform.openai.com/docs/guides/realtime-transcription#realtime-transcription-sessions" target="_blank">這裡</a>。</p>

            <pre class="language-python"><code>
  pipeline = Pipeline(
    [
      transport.input(),
      krisp_filter,
      vad_turn_detector,
      stt,
      context_aggregator.user(), 
      llm, 
      tts, 
      transport.output(), 
      context_aggregator.assistant(),
    ]
  )
            </code></pre>
            <p class="image-caption">包含Krisp處理元素的Pipecat管道</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="voice-activity-detection">4.5.6 語音活動檢測</h3>
            
            <p>語音活動檢測階段是幾乎每個語音AI管道的一部分。VAD將音訊片段分類為「語音」和「非語音」。我們將在下方的<a href="#turn-detection">輪次檢測</a>部分詳細討論VAD。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="network-transport">4.6. 網路傳輸</h2>
            
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="websockets-webrtc">4.6.1 WebSockets和WebRTC</h3>
            
            <p>WebSockets和WebRTC都被AI服務用於音訊串流。</p>
            
            <p>WebSockets非常適合伺服器對伺服器的使用場景。對於延遲不是主要考慮因素的場景，它們也很好用，並且非常適合原型設計和一般開發。</p>
            
            <p>WebSockets不應該在生產環境中用於客戶端-伺服器的即時媒體連接。</p>
            
            <p>如果您正在開發瀏覽器或原生行動應用程式，且對話延遲對您的應用程式很重要，您應該使用WebRTC連接來從您的應用程式發送和接收音訊。</p>
            
            <p>WebSockets用於終端用戶設備的即時媒體傳輸的主要問題是：</p>
            
            <ul class="arrow-list">
              <li>WebSockets建立在TCP上，因此音訊串流會受到隊頭阻塞的影響。</li>
              <li>WebRTC使用的Opus音訊編解碼器與WebRTC的頻寬估計和封包節奏（擁塞控制）邏輯緊密耦合，使WebRTC音訊串流能夠應對各種現實世界的網路行為，這些行為可能會導致WebSocket連接累積延遲。</li>
              <li>Opus音訊編解碼器具有非常好的前向錯誤修正功能，使音訊串流能夠應對相對較高的封包丟失。（但這只有在您的網路傳輸能夠丟棄延遲到達的封包且不進行隊頭阻塞時才有幫助。）</li>
              <li>WebRTC音訊會自動加上時間戳，因此播放和中斷邏輯變得非常簡單。</li>
              <li>WebRTC包含詳細的性能和媒體質量統計的鉤子。一個好的WebRTC平台將為您提供詳細的儀表板和分析。這種級別的可觀察性對於WebSockets來說介於非常困難和不可能之間。</li>
              <li>WebSocket重新連接邏輯很難穩健地實現。您將不得不建立一個ping/ack框架（或完全測試並理解您的WebSocket庫提供的框架）。TCP超時和連接事件在不同平台上的行為不同。</li>
              <li>最後，現今好的WebRTC實現都配備了非常好的回音消除、噪音減少和自動增益控制。</li>
            </ul>

            <p>您可以通過兩種方式使用WebRTC。</p>
            <ol>
              <li>通過雲端中的WebRTC伺服器路由。</li>
              <li>在客戶端設備和語音AI處理程序之間建立直接連接。</li>
            </ol>

            <p>對於許多現實世界的使用場景，通過雲端伺服器路由將表現得更好（參見下方的<a href="#network-routing">網路路由</a>）。雲端基礎設施還使得直接連接不能輕易或可擴展地支援的許多功能成為可能（多參與者會話、與電話系統集成、錄音）。</p>

            <p>但「無伺服器」WebRTC適合許多語音AI使用場景。Pipecat通過<a href="https://docs.pipecat.ai/server/services/transport/small-webrtc" target="_blank">SmallWebRTCTransport</a>類支援無伺服器WebRTC。而像Hugging Face的<a href="https://fastrtc.org/" target="_blank">FastRTC</a>這樣的框架完全圍繞這種網路模式構建。</p>

        </div>
        <div class="chunk-notes">
          <div class="chapter-image-positioned">
            <img src="images/Figure 1900.svg" alt="WebSocket vs WebRTC diagram" class="network-image" width="250">
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="http">4.6.2 HTTP</h3>
            
            <p>HTTP對於語音AI仍然有用且重要！HTTP是互聯網上服務互連的通用語言。REST API是HTTP。Webhooks是HTTP。</p>
            
            <p>文字導向的推理通過HTTP進行，因此語音AI管道通常會調用HTTP API來處理對話循環的LLM部分。</p>
            
            <p>語音代理在與外部服務和內部API集成時也使用HTTP。一個有用的技術是將LLM函數調用代理到HTTP端點。這將語音AI代理程式碼和開發運維與函數實現解耦。</p>
            
            <p>多模態AI應用程式通常會想要實現HTTP和WebRTC代碼路徑。想像一個支援文字模式和語音模式的聊天應用程式。對話狀態需要通過任一連接路徑訪問，這對客戶端和伺服器端代碼都有影響（例如，Kubernetes pods和Docker容器的架構方式）。</p>
            
            <p>HTTP的兩個缺點是延遲和實現長期雙向連接的困難。</p>
            
            <ul class="arrow-list">
              <li>建立加密的HTTP連接需要多次網路往返。即使對於高度優化的伺服器，實現低於30ms的媒體連接設置時間也相當困難，而現實中的首位元組發送時間即使對於高度優化的伺服器也接近100ms。</li>
              <li>長期雙向HTTP連接的管理難度足夠大，以至於您通常最好直接使用WebSockets。</li>
              <li>HTTP是基於TCP的協議，因此影響WebSockets的相同隊頭阻塞問題也是HTTP的問題。</li>
              <li>通過HTTP發送原始二進制數據不夠常見，以至於大多數API選擇對二進制數據進行base64編碼，這增加了媒體串流的比特率。</li>
            </ul>
            
            <p>這就引出了QUIC…</p>

        </div>
        <div class="chunk-notes">
          <div class="chapter-image-positioned image-position-adjust" style="--position-offset: 100px;">
            <img src="images/Figure 2200.svg" alt="HTTP API diagram" class="network-image" width="250">
            <p class="image-caption">一個同時使用HTTP和WebRTC進行網路通信的語音AI代理。</p>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="quic-moq">4.6.3 QUIC和MoQ</h3>
            
            <p>QUIC是一種新的網路協議，設計用作最新版HTTP（HTTP/3）的傳輸層 — 並且靈活地支援其他互聯網規模的使用場景。</p>
            
            <p>QUIC是一種基於UDP的協議，解決了上述所有HTTP問題。使用QUIC，您可以獲得更快的連接時間、雙向串流，以及沒有隊頭阻塞。Google和Facebook一直在穩步推出QUIC，因此現在，您的一些HTTP請求是以UDP而非TCP封包穿越互聯網的。<sup>[24]</sup></p> 
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[24] 如果您已經在互聯網上構建東西很長時間了，這有點令人震驚。HTTP一直是基於TCP的協議！</p>
            </div>
          </div>  
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
             
            <p>QUIC將成為互聯網媒體串流未來的重要部分。但遷移到基於QUIC的即時媒體串流協議需要時間。構建基於QUIC的語音代理的一個障礙是Safari尚未支援基於QUIC的WebSockets演進版本<a href="https://w3c.github.io/webtransport/" target="_blank">WebTransport</a>。</p>
            
            <p>媒體超過QUIC IETF工作組<sup>[25]</sup>旨在開發一個「簡單的低延遲媒體傳輸解決方案，用於媒體的攝取和分發」。與所有標準一樣，用最簡單的構建塊支援最廣泛的重要使用場景並不容易。人們對使用QUIC進行點播視頻串流、大規模視頻廣播、實時視頻串流、具有大量參與者的低延遲會話以及低延遲1:1會話感到興奮。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[25] <a href="https://datatracker.ietf.org/group/moq/about/" target="_blank">IETF媒體超過QUIC工作組</a></p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>即時語音AI使用場景正在以恰到好處的時機增長，可以影響MoQ標準的發展。</p>

        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="network-routing">4.6.4 網路路由</h3>
            
            <p>長距離網路連接對延遲和即時媒體可靠性都是有問題的，無論底層網路協議是什麼。</p>
            
            <p><strong>對於即時媒體傳輸，您希望您的伺服器盡可能靠近您的用戶。</strong></p>
            
            <p>例如，從英國的用戶到AWS us-west-1（北加州）託管的伺服器的封包往返時間通常約為140毫秒。相比之下，同一用戶到AWS eu-west-2的RTT通常為15毫秒或更少。</p>
            <div class="chapter-image">
              <img src="images/Figure 2300.svg" alt="Edge routing diagram" class="network-image" width="700">
              <p class="image-caption">從英國的用戶到AWS us-west-1的RTT比到AWS eu-west-2多約100ms</p>
            </div>
            <p>這是超過100毫秒的差異 — 如果您的語音到語音延遲目標是1,000毫秒，這是您延遲「預算」的百分之十。</p>
            
            <p><strong>邊緣路由</strong></p>
            
            <p>您可能無法在靠近所有用戶的地方部署伺服器。</p>
            
            <p>要在世界各地的用戶實現15ms的RTT，需要部署至少40個全球數據中心。這是一個大型的開發運維工作。而且如果您運行需要GPU的工作負載，或依賴於自身沒有全球部署的服務，這可能是不可能的。</p>
            
            <p>您無法欺騙光速。<sup>[26]</sup> 但您可以嘗試避免路由變化和擁塞。</p>
          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[26] 古老的網路工程師智慧 – 編者註</p>
              </div> 
            </div>
          </div>
        </div>

        <div class="chunk-row">
          <div class="chunk-content">
            <p>關鍵是保持您的公共互聯網路由盡可能短。將您的用戶連接到靠近他們的邊緣伺服器。從那裡，使用私有路由。</p>
            
            <p>這種邊緣路由減少了中位數封包RTT。通過私有骨幹網的英國→北加州路由可能約為100毫秒。100毫秒（長距離私有路由）+ 15毫秒（通過公共互聯網的第一跳）= 115毫秒。這個私有路由中位數RTT比公共路由中位數RTT快25毫秒。</p>
            <div class="chapter-image">
              <img src="images/Figure 2400.svg" alt="Edge routing diagram" class="network-image" width="700">
              <p class="image-caption">從英國到AWS us-west-1的邊緣路由。通過公共網路的第一跳仍有15ms的RTT。但通過私有網路到北加州的長路由有100ms的RTT。總RTT為115ms，比從英國到us-west-1的公共路由快25ms。它也顯著更穩定（更少的封包丟失和更低的抖動）。</p>
            </div>
            <p>比中位數RTT改善更關鍵的是改善的傳輸可靠性和更低的抖動。<sup>[27]</sup> 私有路由的P95 RTT將顯著低於公共路由的P95。<sup>[28]</sup></p>
            
            <p>這意味著通過長距離公共路由的即時媒體連接將明顯比使用私有路由的連接更有延遲。回想一下，我們試圖盡可能快地傳輸每個音訊封包，但我們必須按順序播放音訊封包。單個延遲的封包迫使我們擴大抖動緩衝區，保留其他已接收的封包直到延遲的封包到達。（或者，直到我們決定它花費了太長時間，我們用花哨的數學或有故障的音訊樣本填補空白。）</p>

          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[27] 抖動是封包穿越路由所需時間的變化性。</p>
              </div>
              <div class="footnote">
                <p>[28] P95是指標的第95百分位測量值。P50是中位數測量值（第50百分位）。粗略地說，我們將P50視為平均情況，P95則捕捉「典型最壞情況」連接的大致感覺。</p>
              </div>
            </div>
          </div>
        </div>
        <div class="chunk-row">
          <div class="chunk-content">

            <div class="chapter-image">
              <img src="images/Figure 2500 Figure 4.w.svg" alt="Jitter buffer diagram" class="network-image" width="80%">
              <p class="image-caption">抖動緩衝區 — 較大的抖動緩衝區直接轉化為音訊和視頻中較大的感知延遲。保持抖動緩衝區盡可能小對良好的用戶體驗有顯著貢獻。</p>
            </div>
            <p>一個好的WebRTC基礎設施提供商將提供邊緣路由。他們將能夠向您展示他們的伺服器集群位置，並提供顯示其私有路由性能的指標。</p>

        </div>
        
        <div class="chunk-notes">
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="turn-detection">4.7. 輪次檢測</h2>
            
            <p><em>輪次檢測</em>意味著確定用戶何時完成說話並期望LLM回應。</p>
            
            <p>在學術文獻中，這個問題的各個方面被稱為<em>短語檢測、語音分段和終點檢測</em>。（有學術文獻關於這一點是一個線索，表明這不是一個簡單的問題。）</p>
            
            <p>我們（人類）每次與他人交談時都會進行輪次檢測。而我們並不總是做對！<sup>[29]</sup></p>
            
            <p>因此輪次檢測是一個困難的問題，沒有完美的解決方案。但讓我們談談常用的各種方法。</p>
        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[29] 特別是在音訊通話中，當我們沒有視覺提示幫助我們時。</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="voice-activity-detection-4-7">4.7.1 語音活動檢測</h3>
            
            <p>目前，語音AI代理進行輪次檢測最常見的方式是假設長時間的停頓意味著用戶已經完成說話。</p>
            
            <p>語音AI代理管道使用一個小型專門的語音活動檢測模型來識別停頓。VAD模型已經被訓練來將音訊片段分類為語音或非語音。（這比僅基於音量水平來識別停頓要穩健得多。）</p>
            
            <p>您可以在語音AI連接的客戶端或伺服器端運行VAD。如果您需要在客戶端進行大量音訊處理，您可能需要在客戶端運行VAD來促進這一點。例如，也許您正在嵌入式設備上識別喚醒詞，只有在短語開頭檢測到喚醒詞時才通過網路發送音訊。<em>嘿，Siri…</em></p>
            
            <p>不過，通常在語音AI代理處理循環中運行VAD會更簡單一些。而且如果您的用戶是通過電話連接的，您沒有可以運行VAD的客戶端，所以您必須在伺服器上進行。</p>
            
            <p>語音AI最常用的VAD模型是<a href="https://github.com/snakers4/silero-vad" target="_blank">Silero VAD</a>。這個開源模型在CPU上高效運行，支援多種語言，適用於8khz和16khz音訊，並且作為wasm包可用於網頁瀏覽器。在即時單聲道音訊串流上運行Silero通常只需要典型虛擬機CPU核心的1/8。</p>
            
            <p>輪次檢測算法將有幾個配置參數：</p>
            
            <ul class="arrow-list">
              <li>輪次結束所需的停頓長度。</li>
              <li>觸發開始說話事件所需的語音片段長度。</li>
              <li>將每個音訊片段分類為語音的置信度水平。</li>
              <li>語音片段的最小音量。</li>
            </ul>
        </div>

        <div class="chunk-notes">
          <div class="chapter-image">
            <img src="images/4x.svg" alt="VAD processing step" class="vad-image" width="100%">
            <p class="image-caption">語音活動檢測處理步驟，此處配置為在語音轉文字之前運行</p>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <pre class="language-python"><code>
  # Pipecat's names and default values
  # for the four configurable VAD
  # parameters
  VAD_STOP_SECS = 0.8
  VAD_START_SECS = 0.2
  VAD_CONFIDENCE = 0.7
  VAD_MIN_VOLUME = 0.6
  
</code></pre>
            
            <p>調整這些參數可以大大改善特定使用場景的輪次檢測行為。</p>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="push-to-talk">4.7.2 按鍵通話</h3>
            
            <p>基於語音停頓進行輪次檢測的明顯問題是，有時人們停頓但並未完成說話。</p>
            
            <p>個人說話風格各不相同。人們在某些類型的對話中比其他類型的對話中停頓更多。</p>
            
            <p>設置較長的停頓間隔會創造生硬的對話 — 非常糟糕的用戶體驗。但使用較短的停頓間隔，語音代理會頻繁地打斷用戶 — 同樣是糟糕的用戶體驗。</p>
            
            <p>最常見的替代停頓基礎輪次檢測的方法是按鍵通話。按鍵通話意味著要求用戶在開始說話時按下或按住按鈕，並在完成說話時再次按下按鈕或釋放它。（想想老式對講機的工作方式。）</p>
            
            <p>使用按鍵通話，輪次檢測是明確的。但用戶體驗與純粹交談不同。</p>
            
            <p>電話語音AI代理無法使用按鍵通話。</p>
            
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="endpoint-markers">4.7.3 終點標記</h3>
            
            <p>您也可以使用特定詞語作為輪次結束標記。（想想卡車司機在CB無線電上說「完畢」。）</p>
            
            <p>識別特定終點標記的最簡單方法是對每個轉錄片段運行正則表達式匹配。但您也可以使用小型語言模型來檢測終點詞或短語。</p>
            
            <p>使用明確終點標記的語音AI應用相當罕見。用戶必須學習如何與這些應用交談。但這種方法對於專門的使用場景可以非常有效。</p>
            
            <p>例如，去年我們看到了一個不錯的演示，是某人作為副項目為自己建立的寫作助手。他們使用各種命令短語來指示輪次終點和切換模式。</p>
            
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="context-aware-turn-detection">4.7.4 上下文感知輪次檢測（語義VAD和智能輪次）</h3>
            
            <p>當人類進行輪次檢測時，他們使用各種提示：</p>
            
            <ul class="arrow-list">
              <li>識別像「嗯」這樣的填充詞，這些詞可能表示繼續說話。</li>
              <li>語法結構。</li>
              <li>對模式的了解，例如電話號碼有特定數量的字母。</li>
              <li>語調和發音模式，如在停頓前拉長最後一個詞。</li>
            </ul>
            
            <p>深度學習模型非常擅長識別模式。LLM具有大量潛在的語法知識，可以被提示進行短語終點檢測。較小的專門分類模型可以在語言、語調和發音模式上進行訓練。</p>
            
            <p>隨著語音代理在商業上變得越來越重要，我們預計會看到用於上下文感知語音AI輪次檢測的新模型。</p>
            
            <p>有兩種主要方法：</p>
            
            <ol class="list-decimal">
              <li>訓練一個可以實時運行的小型輪次檢測模型。將此模型與VAD模型結合使用或替代VAD模型。輪次檢測模型可以被訓練來對文字進行模式匹配。文字模式輪次檢測模型在轉錄後的處理管道中內聯運行，通常需要在特定轉錄模型的輸出上進行訓練才能有效。或者，輪次檢測模型可以被訓練為直接在音訊上操作，這允許輪次檢測分類同時考慮語言級別模式和語調、語速和發音模式。原生音訊輪次檢測模型不需要任何轉錄信息，因此可以與轉錄並行運行，這可以提高性能。</li>
              <li>使用大型LLM和少量示例提示來執行輪次檢測。大型LLM通常太慢，無法內聯使用，阻塞管道。為了解決這個問題，您可以分割管道，並行進行輪次檢測和「貪婪」對話推理。</li>
            </ol>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[30] <a href="https://github.com/pipecat-ai/smart-turn" target="_blank">Pipecat開源智能輪次檢測模型</a></p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="interruption-handling">4.8. 中斷處理</h2>
            
            <p><em>中斷處理</em>意味著允許用戶打斷語音AI代理。中斷是對話的正常部分，因此優雅地處理中斷很重要。</p>
            
            <p>要實現中斷處理，您需要讓管道中的每個部分都可以被取消。您還需要能夠在客戶端上非常快速地停止音訊播放。</p>
            
            <p>通常，您正在使用的框架會在觸發中斷時負責停止所有處理。<strong>但如果您直接使用的API以快於實時的速度向您發送原始音訊幀，您將不得不手動停止播放並清空音訊緩衝區。</strong></p>

        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="avoiding-spurious-interruptions">4.8.1 避免虛假中斷</h3>
            
            <p>有幾個值得注意的非預期中斷來源。</p>
            
            <ol class="list-decimal">
              <li>被歸類為語音的瞬態噪音。好的VAD模型在將語音與「噪音」分離方面做得非常出色。但某些類型的短促、尖銳的初始音訊在出現在話語開頭時會附帶中等程度的語音置信度。咳嗽和鍵盤點擊都屬於這一類。您可以調整VAD起始段長度和置信度水平，以嘗試最小化這種中斷來源。權衡是延長起始段長度和提高置信度閾值會為您確實想要檢測為完整話語的非常短的短語創造問題。<sup>[31]</sup></li>
              
              <li>回聲消除失敗。回聲消除算法並不完美。從靜音到語音播放的轉換特別具有挑戰性。如果您做了大量的語音代理測試，您可能聽到過您的機器人在開始說話時打斷了自己。罪魁禍首是回聲消除允許少量初始語音音訊反饋到您的麥克風。最小VAD起始段長度有助於避免這個問題。應用指數平滑<sup>[32]</sup>到音訊音量水平以避免急劇的音量轉換也有幫助。</li>
              
              <li>背景語音。VAD模型不會區分用戶語音和背景語音。如果背景語音比您的音量閾值更大，背景語音將觸發中斷。說話者隔離音訊處理步驟可以減少由背景語音引起的虛假中斷。請參閱上面<a href="#server-side-noise">伺服器端噪音處理和說話者隔離</a>部分的討論。</li>
            </ol>

        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="maintaining-accurate-context">4.8.2 在中斷後維持準確的上下文</h3>
            
            <p>因為LLM生成輸出的速度比實時快，當發生中斷時，您通常會有排隊等待發送給用戶的LLM輸出。</p>
            
            <p>通常，您希望對話上下文與用戶實際聽到的內容匹配（而不是您的管道以快於實時的速度生成的內容）。</p>
            
            <p>您可能也在以文本形式保存對話上下文。<sup>[33]</sup></p>
            
            <p>所以您需要一種方法來確定用戶實際<em>聽到了什麼！</em></p>
            
            <p>最好的語音轉文本服務可以報告詞級時間戳數據。使用這些詞級時間戳來緩衝和組裝與用戶聽到的音訊匹配的助手消息文本。請參閱上面<a href="#text-to-speech">文本轉語音</a>部分關於詞級時間戳的討論。Pipecat會自動處理這個問題。</p>

        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[31] Pipecat的標準管道配置結合了VAD和轉錄事件，以嘗試避免虛假中斷和錯過的話語。</p>
            </div>
            <div class="footnote">
              <p>[32] <a href="https://dub.sh/voice-agents-030" target="_blank">Pipecat VAD輸入音訊指數平滑代碼</a></p>
            </div>
            <div class="footnote">
              <p>[33] 標準上下文結構是由OpenAI開發的用戶/助手消息列表格式。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="managing-conversation-context">4.9. 管理對話上下文</h2>
            
            <p>LLM是無狀態的。這意味著對於多輪對話，您需要在每次生成新回應時將所有先前的用戶和代理消息——以及其他配置元素——重新輸入到LLM中。</p>

            <pre><code>Turn 1:
  <strong>User:</strong> What's the capital of France?
  <strong>LLM:</strong> The capital of France is Paris.

Turn 2:
  <strong>User:</strong> What's the capital of France?
  <strong>LLM:</strong> The capital of France is Paris.
  <strong>User:</strong> Is the Eiffel Tower there?
  <strong>LLM:</strong> Yes, the Eiffel Tower is in Paris.
  
Turn 3:
  <strong>User:</strong> What's the capital of France?
  <strong>LLM:</strong> The capital of France is Paris.
  <strong>User:</strong> Is the Eiffel Tower there?
  <strong>LLM:</strong> Yes, the Eiffel Tower is in Paris.
  <strong>User:</strong> How tall is it?
  <strong>LLM:</strong> The Eiffel Tower is about 330 meters tall.
</code></pre>
<p class="image-caption">每輪向LLM發送整個對話歷史。</p>
              
            
            <p>對於每個推理操作——每個對話輪次——您可以向LLM發送：</p>
            
            <ul class="arrow-list">
              <li>系統指令</li>
              <li>對話消息</li>
              <li>LLM可以使用的工具（函數）</li>
              <li>配置參數（例如，溫度）</li>
            </ul>




        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="differences-between-llm-apis">4.9.1 LLM API之間的差異</h3>
            
            <p>這種一般設計對於今天所有主要的LLM都是相同的。</p>
            
            <p>但各種提供商的API之間存在差異。OpenAI、Google和Anthropic都有不同的消息格式，工具/函數定義結構的差異，以及系統指令指定方式的差異。</p>
            
            <p>有第三方API網關和軟件庫可以將API調用轉換為OpenAI的格式。這很有價值，因為能夠在不同的LLM之間切換很有用。但這些服務並不總是能夠正確地抽象差異。新功能和每個API獨有的功能並不總是受支持。（有時在轉換層中也有錯誤。）</p>
            
            <p>在AI工程的這些相對早期階段，是否抽象仍然是一個問題。<sup>[34]</sup></p>

            <p>例如，Pipecat將上下文消息和工具定義轉換為OpenAI格式並從中轉換。但是否以及如何做到這一點是社區相當大的辯論主題！<sup>[35]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[34] 備註：請Claude想出一個好的哈姆雷特笑話 – 編輯</p>
            </div>
            <div class="footnote">
              <p>[35] 如果您對這類話題感興趣，請考慮加入<a href="https://discord.gg/pipecat" target="_blank">Pipecat Discord</a>並參與那裡的對話。</p>
            </div>
          </div>
        </div>
    </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="modifying-context-between-turns">4.9.2 在輪次之間修改上下文</h3>
            
            <p>必須管理多輪上下文增加了開發語音AI代理的複雜性。另一方面，可以追溯性地修改上下文可能很有用。對於每個對話輪次，您可以決定確切要發送給LLM的內容。</p>
            
            <p>LLM並不總是需要完整的對話上下文。縮短或總結上下文可以減少延遲，降低成本，並提高語音AI代理的可靠性。關於這個主題的更多內容在下面的<a href="#scripting">腳本和指令遵循</a>部分。</p>



        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="function-calling">4.10. 函數調用</h2>
            
            <p>生產級語音AI代理嚴重依賴LLM函數調用。</p>
            
            <p>函數調用用於：</p>
            
            <ul class="arrow-list">
              <li>獲取信息用於檢索增強生成(RAG)。</li>
              <li>與現有後端系統和API交互。</li>
              <li>與電話技術堆棧集成——呼叫轉移、排隊、發送DTMF音調。</li>
              <li>腳本遵循——實現工作流狀態轉換的函數調用。</li>
            </ul>
            
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="function-calling-reliability">4.10.1 語音AI上下文中的函數調用可靠性</h3>
            
            <p>隨著語音AI代理被部署於越來越複雜的用例，可靠的函數調用變得越來越重要。</p>
            
            <p>最先進的LLM在函數調用方面正在穩步改進，但語音AI用例往往將LLM函數調用能力推向極限。</p>
            
            <p>語音AI代理傾向於：</p>
            
            <ul class="arrow-list">
              <li>在多輪對話中使用函數。在多輪對話中，隨著每輪添加用戶和助手消息，提示會變得越來越複雜。這種提示複雜性降低了LLM函數調用能力。</li>
              <li>定義多個函數。通常需要五個或更多函數用於語音AI工作流。</li>
              <li>在一個會話中多次調用函數。</li>
            </ul>
            
            <p>我們對所有主要AI模型發布進行了大量測試，並經常與訓練這些模型的人交談。很明顯，上述所有屬性相對於用於訓練當前一代LLM的數據來說都有些偏離分佈。</p>

            <p>這意味著即使當前一代LLM在一般函數調用基準上表現良好，它們也會在語音AI用例中遇到困難。不同的LLM和同一模型的不同更新在函數調用方面的表現各不相同，在不同情況下對不同類型的函數調用的表現也各不相同。</p>
            
            <p id="function-calling-eval"><strong>如果您正在構建語音AI代理，開發自己的評估來測試應用程序的函數調用性能很重要。請參閱下面的<a href="#evals">語音AI評估</a>部分。</strong></p>
            
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="latency-function-calls">4.10.2 函數調用延遲</h3>
            
            <p>函數調用會增加延遲——可能是很大的延遲——有四個原因：</p>
            
            <ol class="list-decimal">
              <li>當LLM決定需要函數調用時，它會輸出函數調用請求消息。然後您的代碼為特定請求的函數執行操作，然後再次使用相同的上下文加上函數調用結果消息進行推理。所以每次調用函數時，您必須進行兩次推理調用而不是一次。</li>
              <li>函數調用請求不能被流式傳輸。我們需要整個函數調用請求消息才能執行函數調用。</li>
              <li>向提示添加函數定義可能會增加延遲。這有點模糊；開發專門針對延遲的評估來測量向提示添加函數定義的額外延遲會很好。但很明顯，至少有些API在某些時候，無論是否實際調用函數，在啟用工具使用時都有更高的中位數TTFT。</li>
              <li>您的函數可能很慢！如果您正在與遺留後端系統交互，您的函數可能需要很長時間才能返回。</li>
            </ol>
            
            <p>每次用戶說完話，您都需要提供相當快的音訊反饋。如果您知道您的函數調用可能需要很長時間才能返回，您可能想要輸出語音告訴用戶發生了什麼並請他們等待。</p>

          </div>
          <div class="chunk-notes">
            <div class="chapter-image-positioned" data-align-with="function-calling-eval">
              <img src="images/4ad.svg" alt="包含函數調用的推理的TTFT" class="latency-image" width="100%">
              <p class="image-caption">包含函數調用的推理的TTFT。LLM TTFT為450毫秒，吞吐量為每秒100個標記。如果函數調用請求塊為100個標記，則輸出函數調用請求需要1秒。然後我們執行函數並再次運行推理。這次，我們可以流式傳輸輸出，所以450毫秒後我們有了可以使用的第一個標記。完整推理的TTFT為1,450毫秒（不包括執行函數本身所需的時間）。</p>
            </div>
          </div>
        </div>
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>您可以：</p>
            
            <ul class="arrow-list">
              <li>在執行函數調用之前始終輸出消息。「請稍等，我正在為您執行X...」</li>
              <li>設置看門狗計時器，只有在函數調用循環在計時器觸發前未完成時才輸出消息。「仍在處理中，請再等一會兒...」</li>
            </ul>
            
            <p>當然，也可以兩者都用。而且您可以在執行長時間運行的函數調用時播放背景音樂。<sup>[36]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[36] 不過請不要用Jeopardy主題曲。</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="handling-interruptions">4.10.3 處理中斷</h3>
            
            <p>LLM被訓練為期望函數調用請求消息和函數調用響應消息作為匹配的對。</p>
            
            <p>這意味著：</p>
            
            <ol class="list-decimal">
              <li>您需要停止您的語音到語音推理循環，直到所有函數調用完成。請參閱下面關於<a href="#async-function-calls">異步函數調用</a>的說明。</li>
              <li>如果函數調用被中斷且永遠不會完成，您需要在上下文中放入一個函數調用響應消息，表明...某些內容。</li>
            </ol>
            
            <p><strong>這裡的規則是，如果LLM調用函數，您需要在上下文中放入一對請求/響應消息。</strong></p>
            
            <ul class="arrow-list">
              <li>如果您在上下文中放入一個懸而未決的函數調用請求消息，然後繼續多輪對話，您正在創建一個與LLM訓練方式不同的上下文。（有些API不允許這樣做。）</li>
              <li>如果您根本不在上下文中放入請求/響應對，您就是在教LLM（通過上下文學習）不要調用該函數。<sup>[37]</sup>同樣，結果是不可預測的，可能不是您想要的。</li>
            </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[37] 參見論文，<a href="https://arxiv.org/abs/2005.14165" target="_blank">語言模型是少樣本學習者</a>。</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">
            <p>Pipecat通過在每次啟動函數調用時將請求/響應消息對插入上下文來幫助您遵循這些上下文管理規則。（當然，您可以覆蓋此行為並直接管理函數調用上下文消息。）</p>
            
            <p>以下是這種模式的樣子，用於以兩種不同方式配置的函數調用：運行至完成和可中斷。</p>
            
            <pre><code><strong>User:</strong>  Please look up the price of 1000 widgets.
<strong>LLM:</strong> Please wait while I look up the price for 1000 widgets. 
<strong>function call request:</strong> { name: "price_lookup", args: { item: "widget", quantity: 1000 } }
<strong>function call response:</strong> { status: IN_PROGRESS }
</code></pre>
<p class="image-caption">初始上下文消息。函數調用請求消息和函數調用響應佔位符。</p>

            <pre><code><strong>User:</strong>  Please look up the price of 1000 widgets.
<strong>function call request:</strong> { name: "price_lookup", args: { item: "widget", quantity: 1000 } }
<strong>function call response:</strong> { result: { price: 12.35 } }
</code></pre>
<p class="image-caption">函數調用完成時的上下文。</p>

            <pre><code><strong>User:</strong>  Please look up the price of 1000 widgets.
<strong>LLM:</strong> Please wait while I look up the price for 1000 widgets. 
<strong>function call request:</strong> { name: "price_lookup", args: { item: "widget", quantity: 1000 } }
<strong>function call response:</strong> { status: IN_PROGRESS }

<strong>User:</strong> Please lookup the price of 1000 pre-assembled modules.
<strong>LLM:</strong> Please wait while I also look up the price for 1000 pre-assembled modules. 
<strong>function call request:</strong> { name: "price_lookup", args: { item: "pre_assembled_module", quantity: 1000 } }
<strong>function call response:</strong> { status: IN_PROGRESS }
</code></pre>
<p class="image-caption">佔位符允許對話在函數調用運行時繼續進行，而不會「混淆」LLM。
</p>

            <pre><code><strong>User:</strong>  "Please look up the price of 1000 widgets."
<strong>LLM:</strong> "Please wait while I look up the price for 1000 widgets." 
<strong>function call request:</strong> { name: "price_lookup", args: { item: "widget", quantity: 1000 } }
<span class="pre-highlight"><strong>function call response:</strong> { status: CANCELLED }</span>

<strong>User:</strong> Please lookup the price of 1000 pre-assembled modules.
<strong>LLM:</strong> Please wait while I look up the price for 1000 pre-assembled modules.
<strong>function call request:</strong> { name: "price_lookup", args: { item: "pre_assembled_module", quantity: 1000 } }
<strong>function call response:</strong> { status: IN_PROGRESS }
</code></pre>
<p class="image-caption">如果函數調用配置為<strong>可中斷</strong>，則如果用戶在函數調用進行中說話，它將被取消。</p>
            


        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="streaming-mode">4.10.4 流式模式和函數調用塊</h3>
            
            <p>在語音AI代理代碼中，您幾乎總是在流式模式下執行對話推理調用。這使您能夠盡快獲得前幾個內容塊，這對於語音到語音響應延遲很重要。</p>
            
            <p>然而，流式模式和函數調用是一個尷尬的組合。流式傳輸對函數調用塊沒有幫助。在組裝完LLM的完整函數調用請求消息之前，您無法調用函數。<sup>[38]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[38] 如果您使用的是AI框架，該框架可能會對您隱藏這種複雜性。</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>以下是對推理提供商在繼續發展其API時的一些反饋：請提供一種模式，可以原子地傳遞函數調用塊，並與任何流式內容塊隔離。這將顯著降低使用LLM提供商API的代碼的複雜性。</p>
            
          </div>
        </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="execute-function-calls">4.10.5 如何以及在哪裡執行函數調用</h3>
            
            <p>當LLM發出函數調用請求時，您該怎麼做？以下是一些常用的模式：</p>
            
            <ul class="arrow-list">
              <li>直接在您的代碼中執行與請求函數同名的函數調用。這是您在幾乎所有LLM函數調用文檔示例中看到的內容。</li>
              <li>根據參數和上下文將請求映射到操作。將此視為要求LLM進行通用函數調用，您在代碼中進行消歧。這種模式的優點是，如果您給LLM提供少量函數選擇，LLM通常在函數調用方面表現更好。<sup>[39]</sup></li>
              <li>將函數調用代理到客戶端。這種模式在應用程序（非電話）上下文中可用。例如，想像一個get_location()函數。您想要用戶設備的當前位置，因此您需要連接到該設備上的地理查詢API。</li>
              <li>將函數調用代理到網絡端點。這在企業環境中通常是一種特別有用的模式。定義一組與內部API交互的函數。然後在您的代碼中創建一個抽象，將這些函數調用作為HTTP請求執行。</li>
            </ul>
          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[39] 在這裡將函數調用視為一個寬泛的類別——形式上而非口語上的函數。您可以從查找表返回值。您可以運行SQL查詢。</p>
              </div>
            </div>
            <div class="chapter-image">
              <img src="images/4ae.svg" width="100%">
              <p class="image-caption">函數調用模式</p>
            </div>
          </div>

        </div>
      
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="async-function-calls">4.10.6 異步函數調用</h3>
            
            <p>有時您不想立即從函數調用返回。您知道您的函數將需要不可預測的長時間才能完成。也許它根本不會完成。也許您甚至想啟動一個長時間運行的進程，可以隨時間在上下文中添加內容。</p>
            
            <p>想像一個步行旅遊應用程序，讓用戶表達對他們在旅途中可能看到的事物的興趣。「如果我們經過任何著名作家居住過的地方，我特別想聽聽這些。」這種架構的一個很好的方式是讓LLM在用戶表達特定興趣時調用函數。該函數將啟動一個後台進程，在找到與興趣相關的任何內容時將信息注入上下文。</p>
            
            <p><strong>目前使用LLM函數調用無法直接實現這點。函數調用的請求/回應訊息必須一起出現在上下文中。</strong></p>
            
            <p>所以與其定義一個這樣形狀的函數：</p>
            
            <ul class="arrow-list">
              <li><pre><code>register_interest_generator(interest: string) -> Iterator[Message]</code></pre></li>
            </ul>
            
            <p>你需要做類似這樣的事情：</p>
            
            <ul class="arrow-list">
              <li><pre><code>create_interest_task_and_return_success_immediately
  (interest: string, context_queue_callback: Callable[Message]) -> 
    Literal["in_progress", "canceled", "success", "failure"]</code></pre></li>
            </ul>
            
            <p>關於這個主題的更多討論，請參閱下方的<a href="#async-inference-tasks">執行非同步推理任務</a>部分。</p>
            
            <p>隨著LLM和API不斷發展以更好地支援多模態對話使用場景，我們希望看到LLM研究人員探索圍繞非同步函數和作為生成器的長時間運行函數的想法。</p>
            
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
            <h3 id="parallel-composite-function-calling">4.10.7 Parallel and composite function calling</h3>
            
            <p><em>Parallel function calling</em> means that the LLM can request multiple function calls in a single inference response. <em>Composite function calling</em> means that the LLM can flexibly call several functions in a row, chaining functions together to perform complex operations.</p>
            
            <p>These are exciting capabilities!</p>
            
            <p>But they also add to the variability of voice agent behavior. Which means you need to develop evals and monitoring that tests whether parallel and composite function calling is working as expected in real-world conversations.</p>
            
            <p>Handling parallel function calling makes your agent code more complex. We often recommend that people disable parallel function calling unless there is a specific use for it.</p>
            
            <p>Composite function calling feels like magic when it works well. One of our favorite early glimpses of composite function calling was seeing Claude Sonnet 3.5 chain together functions to load resources from files based on filename and timestamp.</p>
            
            <pre><code><strong>User:</strong> Claude, load the most recent picture I have of the Eiffel Tower.
<strong>function call request:</strong> &lt;list_files()&gt;
<strong>function call response:</strong> &lt;['eiffel_tower_1735838843.jpg', 'empire_state_building_1736374013.jpg', 'eiffel_tower_1737814100.jpg', 'eiffel_tower_1737609270.jpg',
'burj_khalifa_1737348929.jpg']
<strong>function call request:</strong> &lt;load_resource('eiffel_tower_1737814100.jpg')&gt;
<strong>function call response:</strong> &lt;{ 'success': 'Image loaded successfully', 'image': … }&gt;
<strong>LLM:</strong> I have loaded an image of the Eiffel Tower. The image shows the Eiffel
Tower on a cloudy day.
</code></pre>
            <p class="image-caption">The LLM figures out how to chain two functions – <strong> list_files()</strong> and <strong>load_resource()</strong> – to respond to a specific instruction. The two functions are described in a tools list. But this chaining behavior is not prompted for.</p>
            
            <p>Composite function calling is a relatively new capability of SOTA LLMs. Performance is "jagged" – surprisingly good, but frustratingly inconsistent.</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="multimodality">4.11. Multimodality</h2>
            
            <p>LLMs now consume and produce audio, images, and video in addition to text.</p>
            
            <p>We talked earlier about <a href="#speech-to-speech">speech-to-speech models</a>. These are models capable of taking audio as input and producing audio as output.</p>
            
            <p>The multimodal capabilities of SOTA models are advancing rapidly.</p>
            
            <p>GPT-4o, Gemini Flash, and Claude Sonnet all have very good vision capabilities – they all accept images as input. Vision support in these models started out focused on describing the image content and transcribing text that appears in images. Capabilities expand with each release. Counting objects, identifying bounding boxes, and better understanding of the relationship between objects in an image are all useful abilities that are available in newer releases.</p>
            
            <p>Gemini Flash can do inference on video input, including understanding both video and audio tracks.<sup>[40]</sup></p>
            
            <p>One interesting new class of voice-enabled applications is the assistant that can "see" your screen and help perform tasks on your local machine or a web browser. A number of people have built scaffolding for voice driven web browsing.</p>
            
            <p>Several programmers we know talk as much as they type, these days. It's fairly easy to wire up voice input to drive Cursor or Windsurf.<sup>[41]</sup> It's also possible to wire up screen capture so your AI programming assistant can see exactly what you see – code in your editor, UI state of the web app you're building, a Python stacktrace in your terminal. This kind of fully multimodal AI programming assistant feels like another of the glimpses of the future we've talked about throughout this document.<sup>[42]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[40] You can process video with both GPT-4o and Claude by extracting individual frames from video and embedding those frames in the context as images. This approach has limitations, but works well for some "video" use cases.</p>
            </div>
            <div class="footnote">
              <p>[41] Two popular new programming editors with deep AI integration and tooling.</p>
            </div>
            <div class="footnote">
              <p>[42] See swyx's talk at OpenAI Dev Day 2024 Singapore,  <a href="https://dub.sh/voice-agents-040" target="_blank">"Engineering AI Agents"</a>.</p>
            </div>
          </div>
        </div>

      </div>
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>Right now, all the SOTA models support multimodality in different combinations.</p>
            
            <ul class="arrow-list">
              <li>GPT-4o (gpt-4o-2024-08-06) has text and image input, and text output.</li>
              <li>gpt-4o-audio-preview has text and audio input, and text and audio output. (No image input.)</li>
              <li>Gemini Flash has text, audio, image, and video input, but only text output.</li>
              <li>OpenAI's new speech-to-text and text-to-speech models are fully steerable and built on the gpt-4o foundation, but are specialized for converting between text and audio: gpt-4o-transcribe, gpt-4o-mini-transcribe, and gpt-4o-mini-tts. </li>
            </ul>
            
            <p>Multimodal support is evolving rapidly, and we expect the above list to be out of date soon!</p>
            
            <p>For voice AI, the biggest challenge with multimodality is that audio and images use a lot of tokens, and more tokens mean higher latency.</p>
            
            <table class="data-table">
              <tr>
                <th>Example media</th>
                <th>Approximate token count</th>
              </tr>
              <tr>
                <td>One minute of speech audio as text</td>
                <td>150</td>
              </tr>
              <tr>
                <td>One minute of speech audio as audio</td>
                <td>2,000</td>
              </tr>
              <tr>
                <td>One image</td>
                <td>250</td>
              </tr>
              <tr>
                <td>One minute of video</td>
                <td>15,000</td>
              </tr>
            </table>
            
            <p>For some applications, a big engineering challenge is achieving conversational latency while also handling large numbers of images. Conversational latency requires either keeping the context small or relying on vendor-specific caching APIs. Images add a lot of tokens to the context.</p>
            
            <p>Imagine a personal assistant agent that runs all the time on your computer and watches your screen as part of its work loop. You might like to be able to ask, "I was about to read a tweet an hour ago when I got that phone call, and then I forgot about it and closed the tab. What was that tweet?"</p>
            
            <p><em>An hour ago equates to almost a million tokens.</em> Even if your model can accommodate a million tokens in its context<sup>[43]</sup>, the cost and the latency of doing a multi-turn conversation with that many tokens every turn are prohibitive.</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[43] Hello, Gemini!</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>You can summarize video as text, and keep only the summary in the context. You can calculate embeddings and do RAG-like lookup. LLMs are quite good at both feature summarization and using function calling to trigger complex RAG queries. But both of those approaches are complicated to engineer.</p>
            
            <p>Ultimately, the biggest lever is context caching. All the SOTA API providers offer some support for caching. None of today's caching features are perfect, yet, for voice AI use cases. We expect caching APIs to improve this year, as multimodal, multi-turn conversation use cases get more attention from people training SOTA models.</p>

        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="multiple-models">5. Using multiple AI models</h1>
            
            <p>Today's production voice AI agents use multiple deep learning models in combination.<sup>[44]</sup></p>
            
            <p>As we've discussed, the typical voice AI processing loop transcribes the user's voice with a speech-to-text model, passes the transcribed text to an LLM to generate a response, then performs a text-to-speech step to generate the agent's voice output.</p>
            
            <p>In addition, many production voice agents today use multiple models in complex and varied ways.</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[44] Even the beta speech-to-speech APIs from OpenAI and Google use dedicated VAD and noise reduction models to implement turn detection.</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="fine-tuned-models">5.1. Using several fine-tuned models</h2>
            
            <p>Most voice AI agents use a SOTA<sup>[45]</sup> model from OpenAI or Google (and sometimes Anthropic or Meta). Using the newest, best-performing models is important because voice AI workflows generally are right at the edge of the <em>jagged frontier</em><sup>[46]</sup> of model capability. Voice agents need to be able to follow complex instructions, participate in open-ended conversations with people in a natural way, and use functions and tools reliably.</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[45] SOTA — state of the art — is a widely used AI engineering term that loosely means "the newest large models from the leading AI labs."</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>But for some specialized use cases, it can make sense to fine-tune models for different states of a conversation. A fine-tuned model can be smaller, faster, and cheaper to run than a large model while still performing equally well (or better) on specific tasks.</p>
            
            <p>Imagine an agent that assists with parts ordering from a very large industrial supply catalog. For this task, you might train several different models, each one focused on a different category: plastic materials, metal materials, fasteners, plumbing, electrical, safety equipment, etc.</p>

          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[46] Wharton professor <a href="https://x.com/emollick" target="_blank">Ethan Mollick</a> coined the term "jagged frontier" to describe the complex edge zone of SOTA model capability — sometimes astonishingly good, sometimes frustratingly bad.</p>
              </div>
            </div>
          </div>
        </div>
      <div class="chunk-row">
        <div class="chunk-content">          
            
            <p>Fine-tuned models can generally "learn" things in two important categories:</p>
            
            <ol class="list-decimal">
              <li>Embedded knowledge — models can learn facts.</li>
              <li>Response patterns — models can learn to transform data in specific ways, which also includes learning conversational patterns and flows.</li>
            </ol>
            
            <p>Our hypothetical industrial supply company has extensive raw data:</p>
            
            <ul class="arrow-list">
              <li>A very large knowledge base consisting of data sheets, manufacturer recommendations, prices, and internal data about every part in the catalog.</li>
              <li>Text chat logs, email chains, and transcribed phone conversations with human support agents.</li>
            </ul>
            <div class="chapter-image">
              <img src="images/5a.svg" alt="Using fine-tuned models" class="chunk-image-inline" width="60%">
              <p class="image-caption">Using fine-tuned models for specific conversation topics. A variety of architectural approaches are possible. In this example, at the beginning of each conversation turn a router LLM classifies the full context.</p>
            </div>
            
            <p>Turning this raw data into data sets for fine-tuning models is a large job, but tractable. The required data cleaning, data set creation, model training, and model evaluation are all well-understood problems.</p>
            
            <p><strong>One important note: don't jump straight to fine-tuning — start with prompt engineering.</strong></p>
            
            <p>Prompting can almost always achieve the same task results as fine-tuning. The advantage of fine-tuning is the ability to use a smaller model, which can translate to faster inference and lower cost.<sup>[47]</sup></p>
            
            <p>With prompting, you can get started much more easily and iterate much more quickly than you can with fine-tuning.<sup>[48]</sup></p>

            <p>When initially exploring how to use different models for different conversation states, think of your prompts as miniature "models." You are teaching the LLM what to do by crafting a large, context-specific prompt.</p>

            <ol class="list-decimal">
              <li>For embedded knowledge, implement a search capability that can pull information from your knowledge base and assemble search results into an effective prompt. For more on this, see the <a href="#rag-memory">RAG and memory</a> section, below.</li>
              <li>For response patterns, embed examples of how you expect the model to respond to different questions. Sometimes, just a few examples are enough. Sometimes, you will need lots of examples — 100 or more.</li>
            </ol>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[47] If you're interested in digging deep into prompting vs fine-tuning, see these two classic papers: Language Models Are Few-shot Learners, and A Comprehensive Survey of Few-shot Learning.</p>
            </div>
            <div class="footnote">
              <p>[48] Follow the classic engineering advice: make it work, make it fast, make it cheap. Don't think about moving from prompt engineering to fine-tuning until somewhere in the middle of the make it <em>fast</em> part of the process. (If at all.)</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="async-inference-tasks">5.2. Performing async inference tasks</h2>
            
            <p>Sometimes you want to use an LLM for a task that will take a relatively long time to run. Remember that in our core conversation loop we're aiming for response times of around a second (or less). If a task will take longer than a couple of seconds, you have two choices:</p>
            
            <ol class="list-decimal">
              <li>Tell the user what's happening and ask them to wait. <em>Please hold on while I look that up for you …"</em></li>
              <li>Perform the longer task asynchronously, allowing the conversation to continue while it's happening in the background. <em>"I'll look that up for you. While I do that, do you have any other questions?"</em></li>
            </ol>
            
            <p>If you're performing an inference task asynchronously, you might choose to use a different LLM for that specific task. (Since it's decoupled from the core conversation loop.) You might use an LLM that is slower than would be acceptable for voice responses, or an LLM you have fine-tuned for a specific task.</p>

            <p>A few examples of async inference tasks:</p>
            
            <ul class="arrow-list">
              <li>Implementing content "guardrails". (See the <a href="#content-guardrails">Content guardrails</a> section.)</li>
              <li>Creating an image.</li>
              <li>Generating code to run in a sandbox.</li>
            </ul>
            
            <p>The amazing recent progress in reasoning models<sup>[49]</sup> expands what we can ask LLMs to do. You can't use these models for a voice AI conversation loop, though, because they will often spend significant time producing thinking tokens before they emit usable output. Using reasoning models as async parts of a multi-model voice AI architecture can work well, though.</p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[49] Examples of reasoning models include DeepSeek R1, Gemini Flash 2.0 Thinking, and OpenAI o3-mini.</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="chunk-row">
        <div class="chunk-content">
             
            <p>Async inference is usually triggered by an LLM function call. A simple approach is to define two functions.</p>

            <ul class="arrow-list">
              <li><code>perform_async_inference()</code> — This is called by the LLM when it decides that any long-running inference task should run. You can define more than one of these. Note that you need to start the async task and then immediately return a basic <em>started task successfully</em> response, so that the function call request and response messages are correctly ordered in the context.<sup>[50]</sup></li>
              <li><code>queue_async_context_insertion()</code> — This is called by your orchestration layer when your async inference finishes. The tricky thing here is that how you insert results into the context will depend on what you're trying to do, and on what the LLM/API you are using allows. One approach is to wait until the end of any in-progress conversation turn (including the completion of all function calls), put the async inference results into a specially crafted user message, and then run another conversation turn.</li>
            </ul>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[50] See <a href="#async-function-calls">Asynchronouns function calls.</a></p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="content-guardrails">5.3. Content guardrails</h2>
            
            <p>Voice AI agents have several vulnerabilities that cause major issues for some use cases.</p>
            
            <ul class="arrow-list">
              <li>Prompt injection</li>
              <li>Hallucination</li>
              <li>Out-of-date knowledge</li>
              <li>Production of inappropriate or unsafe content</li>
            </ul>
            
            <p><em>Content guardrails</em> is a general term for code that tries to detect any of these — protecting the LLM from both accidental and malicious prompt injection; catching bad LLM output before it is sent to users.</p>
            
            <p>Using a specific model (or models) for guardrails has a couple of potential advantages:</p>
            
            <ul class="arrow-list">
              <li>Small models can be a good fit for guardrails and safety monitoring. Identifying problematic content can be a relatively specialized task. (In fact, for prompt injection mitigation specifically, you don't necessarily want a model that can be prompted in a fully general way.)</li>
              <li>Using a different model for guardrail work has the advantage that it won't have exactly the same weaknesses as your main model. At least in theory.</li>
            </ul>
            
            <p>Several open source agent frameworks have guardrails components.</p>
            
            <ul class="arrow-list">
              <li>llama-guard is part of Meta's <a href="https://github.com/facebookresearch/llama-stack" target="_blank">llama-stack</a></li>
              <li><a href="https://github.com/NVIDIA/NeMo-Guardrails" target="_blank">NeMO Guardrails</a> is an open-source toolkit for adding programmable guardrails to LLM-based conversational applications</li>
            </ul>
            
          <div class="chapter-image">
            <img src="images/5b.svg" alt="NeMo Guardrails framework" class="chunk-image-inline" width="60%">
            <p class="image-caption">Five types of guardrails supported by NVIDIA's NeMo Guardrails framework. Diagram from NeMo Guardrails documentation.</p>
          </div>
            <p>Both of these frameworks were designed with text chat in mind, not voice AI. But both have useful ideas and abstractions and are worth looking at if you are thinking about guardrails, safety, and content moderation.</p>
            
            <p><strong>It's worth noting that LLMs are much, much better at avoiding all of these issues than they were a year ago.</strong></p>
            
            <p>Hallucination in general is not a major issue any more with the newest models from the large labs. We only see two categories of hallucination regularly, these days.</p>
            
            <ul class="arrow-list">
              <li>The LLM "pretending" to call a function, but not actually doing so. This is fixable through prompting. You need good evals to be sure there aren't cases where this happens with your prompts. When you see function call hallucination in your evals, iterate on your prompt until you don't see it any more. (Remember that multi-turn conversations <em>really</em> stress LLM function calling abilities, so your evals need to mirror your real-world, multi-turn conversations.)</li>
              <li>The LLM hallucinating when you expect it to do a web search. Built-in search grounding is a relatively new feature of LLM APIs. It's a little bit unpredictable, still, whether LLMs will choose to perform a search. If they don't search, they may respond with (older) knowledge embedded in their weights, or a hallucination. Unlike function call hallucination, this is not particularly easy to fix with prompting. But it is easy to know whether a search was actually performed. So you can display that information in an application UI or inject it into the voice conversation. If your app relies on web search, doing this is a good idea. You're pushing the problem to the user to understand and deal with, but that's better than hiding the "searched" or "didn't search" distinction from the user. On the positive side, when search grounding does work, it can largely eliminate out-of-date knowledge issues.</li>
            </ul>
            
            <p>All of the APIs from the major labs have very good content safety filters.</p>
            
            <p>Prompt injection mitigation is also much better than it was a year ago, but the surface area of potential prompt injection attacks expands as LLMs gain new capabilities. For example, prompt injection from text in images is now an issue.</p>
            
            <p>As a very, very general guideline: today in voice AI use cases you are unlikely to see occurrences of accidental prompt injection caused by normal user behavior. But it is definitely possible to steer LLM behavior in ways that subvert system instructions, solely through user input. It's important to test your agents with this in mind. <strong>In particular, it's very important to sanitize and cross-check LLM-generated input to any functions that access backend systems.</strong></p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="single-inference-actions">5.4. Performing single inference actions</h2>
            
            <p>For AI engineers, learning how to leverage LLMs is an ongoing process. Part of that process is a mental shift in how we think about these new tools. When we first started using LLMs, most of us thought about them through the lens, <em>what are language models uniquely capable of?</em> But LLMs are general-purpose tools. They are good at a very broad range of information processing tasks.</p>
            
            <p>In a voice agent context, we always have a code path set up to perform LLM inference. We don't need to limit ourselves to using the LLM only for the core conversation loop.</p>
            
            <p>For example:</p>
            
            <ul class="arrow-list">
              <li>Any time you reach for a regular expression, you can probably write a prompt instead.</li>
              <li>Post-processing LLM output is often useful. For example, you might want to generate output in two formats: text for display in a UI and voice for the interactive conversation. You can prompt the conversation LLM to generate nicely formatted markdown text, then prompt the LLM again to shorten and reformat the text for voice generation.<sup>[51]</sup></li>
              <li>Recursion is powerful.<sup>[52]</sup> You can do things like have an LLM generate a list, and then call the LLM again to perform operations on each element of the list.</li>
              <li>It turns out that you often want to summarize multi-turn conversations. LLMs are fantastic, steerable, summarizers. More on this in the <a href="#scripting">Scripting and instruction following</a> section, below.</li>
            </ul>

          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[51] See also the <a href="#content-guardrails">Content guardrails</a> section, above, regarding post-processing LLM output.</p>
              </div>
              <div class="footnote">
                <p>[52] We're programmers, of course we&nbsp;… — ed.</p>
              </div>
            </div>
          </div>
        </div>
      <div class="chunk-row">
        <div class="chunk-content">            
            
            <p>Many of these emerging code patterns look like a language model using either itself, or another language model, as a tool.</p>
            
            <p>This is such a powerful idea that we expect to see lots of people work on this in 2025. Agent frameworks can build support for this into their library-level APIs. Models can be trained to perform inference recursively in a way roughly analogous to training them to call functions and perform code execution.</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="self-improving-systems">5.5. Towards self-improving systems</h2>
            
            <p>When we access a SOTA "model" via an API, we are not accessing a single artifact. The systems behind the APIs use various routing, multi-stage processing, and distributed systems techniques to perform inference fast, flexibly, reliably, and at extraordinary scale. These systems are always being tweaked. Weights are updated. Low-level inference implementations get more efficient all the time. Systems architectures evolve.</p>
            
            <p>The big labs are continually shortening the feedback loop between how users use their APIs and how they implement inference and other capabilities.</p>
            
            <p>These ever-faster feedback loops are a big part of the amazing macro-level AI progress happening these days.</p>
            
            <p>Taking inspiration from this, what could micro-level feedback loops in our agent-level code look like? Can we build specific scaffolding that improves agent performance during a conversation?</p>
            
            <ul class="arrow-list">
              <li>Monitor how often the agent interrupts the user before they are finished talking, and dynamically adjust parameters like VAD timeouts.</li>
              <li>Monitor how often the user interrupts the agent and dynamically adjust LLM response length.</li>
              <li>Look for patterns that indicate a user is having trouble understanding the conversation — maybe the user is not a native speaker. Adjust the conversation style or offer to switch languages.</li>
            </ul>
            
            <p>Can you think of other ideas?</p>

        </div>
        
        <div class="chunk-notes">

          <div class="chapter-image">
            <pre><code><strong>User:</strong> How has MNI performed recently?
<strong>Agent:</strong> The Miami Dolphins won their game yesterday 21
to 3 and now lead the AFC East with two games remain-
 ing in the regular season.
<strong>User:</strong> No, I meant the stock MNI.
<strong>Agent:</strong> Ah, my apologies! You're asking about the
stock performance of MNI, which is the ticker symbol
 for McClatchy Company …
 From this point on, the model will bias towards
interpreting phonemes or transcribed text as "MNI"
 rather than "Miami".
            </code></pre>
            <p class="image-caption">An example of an LLM adjusting behavior based on user feedback during a multi-turn session (in-context learning)</p>
          </div>
          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="scripting">6. Scripting and instruction following</h1>
            
            <p>A year ago, just being able to build voice agents capable of open-ended conversations at natural human latency was exciting.</p>
            
            <p>Now we're deploying voice AI agents to do complicated, real-world tasks. For today's use cases, we need to instruct the LLM to focus on specific goals during a session. Often, we need the LLM to perform sub-tasks in a specific order.</p>
            
            <p>For example, in a healthcare patient intake workflow, we want the agent to:</p>
            
            <ul class="arrow-list">
              <li>Verify the patient's identity before doing anything else.</li>
              <li>Make sure to ask what medications the patient is currently taking.</li>
              <li>If the patient says they are taking medicine X, ask a particular follow-up question.</li>
              <li>Etc …</li>
            </ul>
            
            <p>We refer to crafting step-by-step workflows as <em>scripting</em>. One lesson from the last year of voice AI development is that it's hard to achieve scripting reliability with <em>prompt engineering</em> alone.</p>
            
            <p>There's only so much detail that can be packed into a single prompt. Relatedly, as the context grows in a multi-turn conversation, the LLM has more and more information to keep track of, and instruction following accuracy declines.</p>
            
            <p>Many voice AI developers are moving towards a state machine approach to building complex workflows. Instead of writing a long, detailed system instruction to guide the LLM, we can define a series of states. Each state is:</p>
            
            <ul class="arrow-list">
              <li>A system instruction and tools list.</li>
              <li>A conversation context.</li>
              <li>One or more exits from the current state to another state.</li>
            </ul>
            
            <p>Each state transition is an opportunity to:</p>
            
            <ul class="arrow-list">
              <li>Update the system instruction and tools list.</li>
              <li>Summarize or modify the context.<sup>[53]</sup></li>
            </ul>
          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[53] Usually, you make an LLM inference call to perform context summarization. :-)</p>
              </div>
            </div>
          </div>
        </div>
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>The state machine approach works well because shorter, more focused system instructions, tools lists, and contexts significantly improve LLM instruction following.</p>
            
            <p>The challenge is to find the right balance between, on the one hand, leveraging the LLM's ability to have an open-ended, natural conversation, and on the other, making sure the LLM reliably executes the important parts of the job to be done.</p>
            
            <p><a href="https://github.com/pipecat-ai/pipecat-flows" target="_blank">Pipecat Flows</a> is a library built on top of Pipecat that helps developers create workflow state machines.</p>
            
            <p>The state diagram is represented as JSON and can be loaded into a Pipecat process. There's a graphical editor for creating these JSON state diagrams.</p>
            <div class="chapter-image">
              <img src="images/6a.png" alt="Pipecat Flows graphical editor" class="chunk-image-inline" width="80%">
              <p class="image-caption">Pipecat Flows graphical editor</p>
            </div>
            <p>Pipecat Flows and state machines are seeing a lot of developer adoption right now. But there are other interesting ways to think about building abstractions for complex workflows.</p>
            
            <p>One active area of AI research and development is multi-agent systems. You could think of a workflow as a multi-agent system, instead of as a series of states to traverse.</p>
            
            <p>One of Pipecat's core architectural components is the parallel pipeline. A parallel pipeline allows you to split the data going through the processing graph and operate on it twice (or more). You can block and filter data. You can define many parallel pipelines. You could think of a workflow as a set of gated, coordinated parallel pipelines.</p>
            
            <p>The rapid evolution of voice AI tooling is exciting, and highlights how early we are in figuring out the best way to build these new kinds of programs.</p>

        </div>
        
        <div class="chunk-notes">

          <div class="chunk-footnotes">

          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="evals">7. Voice AI Evals</h1>
            
            <p>One very important type of tooling is the eval, short for evaluation.</p>
            
            <p><em>Eval</em> is a machine learning term for a tool or process that assesses the capabilities of a system and judges its quality.</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="evals-different">7.1. Voice AI evals are different from software unit tests</h2>
            
            <p>If you're coming from a traditional software engineering background, you're used to thinking about testing as a (mostly) deterministic exercise.</p>
            
            <p>Voice AI requires tests that are different from traditional software engineering. Voice AI outputs are non-deterministic. The inputs for testing voice AI are complex, branching, multi-turn conversations.</p>
            
            <p>Instead of testing that a specific input produces a specific output <code>(f(x) = y)</code>, you will need to run probabilistic evals – lots of test runs to see how often a certain type of event happens.<sup>[54]</sup> For some tests, getting a class of cases right 8/10 times is acceptable, for others accuracy needs to be 9.99/10.</p>
          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[54] The user request was fulfilled, the agent interrupted the user, the agent went off topic, etc</p>
              </div>
            </div>
          </div>
        </div>
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>Instead of just having one input, you will have many: all of the user responses. This makes it very hard to test voice AI applications without attempting to simulate user behavior.</p>
            
            <p>Finally, voice AI tests have non-binary results and will rarely yield a definitive ✅ or ❌ like traditional unit tests do. Instead, you will need to review results and decide on tradeoffs.</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="failure-modes">7.2. Failure modes</h2>
            
            <p>Voice AI apps have particular shapes and failure modes that influence how we design and run evals. Latency is critical (so latency that would be acceptable in a text-mode system is a failure for a voice system). They are multi-model (poor performance could be caused by TTS instability rather than LLM behavior, for example).</p>
            
            <p>Some areas that frequently present challenges today are:</p>
            
            <ul class="arrow-list">
              <li>Latency of time to first speech and time to agent response</li>
              <li>Transcription errors</li>
              <li>Understanding and verbalizing addresses, emails, names, phone numbers</li>
              <li>Interruptions</li>
            </ul>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="eval-strategy">7.3. Crafting an eval strategy</h2>
            
            <p>A rudimentary eval process can be as simple as a spreadsheet with prompts and test cases.</p>
            
            <p>One typical approach is to run each prompt whenever you test a new model or change a major part of your system, using an LLM to judge whether the responses fall within some definition of expected parameters.</p>

            <p>Having a basic eval is much better than not having any evals at all. But investing in evals – having really good evals – becomes critical as you start to operate at scale.</p>

        </div>
        
        <div class="chunk-notes">

          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <p>Evaluation platforms that offer sophisticated tooling for voice AI use cases are just beginning to emerge. Three platforms that have invested early in specific workflows and tools for audio evals are <a href="https://coval.dev" target="_blank">Coval</a>, <a href="https://freeplay.ai/" target="_blank">FreePlay</a>, and <a href="https://wandb.ai/site/weave/" target="_blank">Weights & Biases Weave</a>. All three have good Pipecat integrations.

          <div class="chapter-image">
            <img src="images/7a.jpg" alt="A screenshot from the Coval evals platform UI" class="chunk-image-inline" width="80%">
            <p class="image-caption">A screenshot from the Coval evals platform UI</p>
          </div>
            
            <p>These platforms can help with:</p>
            
            <ul class="arrow-list">
              <li>Prompt iteration.</li>
              <li>Off-the-shelf metrics for audio, workflow, function calling, and semantic evaluation of conversations.</li>
              <li>Hillclimbing on problem areas (for example, making your agents better at handling interruptions).</li>
              <li>Regression testing (to be sure when you fix one problem area you don't introduce regressions in other previously solved problem areas).</li>
              <li>Tracking performance changes over time, both as changes are made by developers, and across user cohorts.</li>
            </ul>

        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">

          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="telephony">8. Integrating with telephony infrastructure</h1>
            
            <p><strong>Most of the fastest growing voice AI use cases today involve telephone calls.</strong> AI voice agents are answering phone calls and making phone calls at scale today.</p>
            
            <p>Some of this is happening in traditional call centers. Call centers mostly view voice AI as a technology that can improve "deflection rates" – the percentage of calls that can be handled by automation rather than human agents. This makes the ROI for adopting voice AI clear. If the per-minute cost of an LLM is cheaper than the per-minute cost of a human agent, the buying decision is easy.<sup>[55]</sup></p>
          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[55] Assuming, of course, that AI agent performance is good. Which, for a wide variety of customer support use cases today, it is.</p>
              </div>
            </div>
          </div>
        </div>
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>A couple of interesting things are happening that accelerate adoption, though, beyond simple ROI calculations.</p>
            
            <p>Voice AI agents are scalable in ways that a human staff isn't. Once you have voice AI in place, wait times during high-volume periods go down. (Customer satisfaction scores go up, as a direct result.)</p>
            
            <p>And LLMs can sometimes do a better job than human agents because we're giving them better tools. In many customer support situations, human agents have to deal with multiple legacy backend systems. Finding information in a timely fashion can be a challenge. When we deploy voice AI into that same situation, we have to build API-level access to these legacy systems. New LLM-plus-API layers are enabling the technology transition to voice AI.</p>
            
            <p>It's clear that generative AI is going to completely reshape the call center landscape over the next few years.</p>
            
            <p>Outside the call center, voice AI is changing how small businesses field phone calls, and how they use phone calls for information discovery and coordination. We talk every day to startups building specialized AI telephony solutions for every business vertical that you've ever heard of.</p>
            
            <p>People in this space often joke that pretty soon humans won't make, or receive, phone calls at all. The phone calls will all be AI-to-AI. Judging from the trendlines we see, there's some truth to this!</p>
            
            <p>If you're interested in telephony for voice AI, there are a few acronyms and common ideas you should be familiar with.</p>
            
            <ul class="arrow-list">
              <li>PSTN is the <em>public, switched, telephone network</em>. If you need to interact with a real phone that has a phone number, you'll need to work with a PSTN platform. Twilio is a PSTN platform that almost every developer has heard of.</li>
              
              <li>SIP is a specific protocol used for IP telephony, but in a general sense SIP is used to refer to telephone interconnects between systems. If you're interfacing with a call center tech stack, for example, you'll need to use SIP. You can work with a SIP provider, or host your own SIP servers.</li>
              
              <li>DTMF tones are the keypress sounds used to navigate telephone menus. Voice agents need to be able to send DTMF tones to interact with real-world telephone systems. LLMs are pretty good at dealing with phone trees. You just need to do a little bit of prompt engineering and define functions that send DTMF tones.</li>
              
              <li>Voice agents often need to execute call transfers. In a simple transfer, the voice AI exits the session by calling a function that triggers a call transfer.<sup>[56]</sup> A <em>warm transfer</em> is a hand-off from one agent to another, in which the agents talk to each other before transferring the caller to the second agent. Voice AI agents can do warm transfers, just like humans can. The voice agent starts out talking to the human caller, then puts the human caller on hold and has a conversation with the new human agent being brought into the call, then connects the human caller to the human agent.</li>
            </ul>

        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[56] The actual transfer operation might be an API call to your telephony platform, or a SIP REFER action.</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="rag-memory">9. RAG and memory</h1>
            
            <p>Voice AI agents often access information from external systems. For example, you might need to:</p>
            
            <ul class="arrow-list">
              <li>Incorporate information about the user into the LLM system instructions.</li>
              <li>Retrieve previous conversation history.</li>
              <li>Look up information in a knowledge base.</li>
              <li>Perform a web search.</li>
              <li>Do a realtime inventory or order status check.</li>
            </ul>
            
            <p>All of these fall under the category of RAG – retrieval augmented generation. RAG is the general AI engineering term for combining information retrieval and LLM prompting.</p>
            
            <p>The "simplest possible RAG" for a voice agent is looking up information about a user before the conversation starts, then merging that information into the LLM system instructions.</p>
            
            <pre><code>user_info = fetch_user_info(user_id)

system_prompt_base = "You are a voice AI assistant..."

system_prompt = (
  system_prompt_base
  + f"""
The name of the patient is {user_info["name"]}.
The patient is {user_info["age"]} years old.
The patient has the following medical history: {user_info["summarized_history"]}.
"""
)
</code></pre>
            <p class="image-caption">Simple RAG – perform a lookup at the beginning of the session</p>
            
            <p>RAG is a deep topic and an area of rapid change.<sup>[57]</sup> Techniques range from the relatively simple approach above that just uses basic lookups and string interpolation, to systems that organize very large amounts of semi-structured data using embeddings and vector databases.</p>

          </div>
          <div class="chunk-notes">
            <div class="chunk-footnotes">
              <div class="footnote">
                <p>[57] Hmm. This sounds like every other area of generative AI, these days.</p>
              </div>
            </div>
          </div>
        </div>
      <div class="chunk-row">
        <div class="chunk-content">
            
            <p>Often, an 80/20 approach gets you a very long way. If you have an existing knowledge base, use the APIs you already have. Write simple evals so you can test a few different formats for injecting lookup results into the conversation context. Deploy to production, then monitor how well this works with real-world users.</p>
            
            <pre><code>async def query_order_system(function_name, tool_call_id, args, llm, context, result_callback):
  "First push a speech frame. This is handy when the LLM response might take a while."
  await llm.push_frame(TTSSpeakFrame("Please hold on while I look that order up for you."))

  query_result = order_system.get(args["query"])
  await result_callback({
    "info": json.dumps({
     "lookup_success": True,
     "order_status": query_result["order_status"],
     "delivery_date": query_result["delivery_date"],
    })
 })

llm.register_function("query_order_system", query_order_system)
</code></pre>
            <p class="image-caption">RAG during a session. Define a function for the LLM to call when information lookup is required. In this example, we also emit a pre-set spoken phrase to let the user know the system will take a few seconds to respond.</p>
            
            <p>As always, latency is a bigger challenge with voice AI than for non-voice AI systems. When an LLM makes a function call request, the extra inference call adds to latency. Looking up information in external systems can be slow, too. It's often useful to trigger a simple speech output before executing the RAG lookup, to let the user know that work is in progress.</p>
            
            <p>More broadly, memory across sessions is a useful capability. Imagine a voice AI personal assistant that needs to remember everything you talk about. Two general approaches are:</p>
            
            <ol class="list-decimal">
              <li>Save each conversation to persistent storage. Test a few approaches to loading conversations into the context. For example, a strategy that works well for the personal assistant use case: always load the most recent conversation in full at agent startup, load summaries of the most recent N conversations, and define a lookup function the LLM can use to load older conversations dynamically as needed.</li>
              
              <li>Save each message in the conversation history separately in a database, along with metadata about the message graph. Index every message (perhaps using semantic embeddings). This allows you to build branching conversation histories dynamically. You might want to do this if your app makes heavy use of image input (LLM vision). Images take up a lot of context space!<sup>[58]</sup> This approach also allows you to build branching UIs, which is a direction that AI app designers are just starting to explore.</li>
            </ol>

        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">

            <div class="footnote">
              <p>[58] See <a href="#multimodality">Multimodality</a>.</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="hosting">10. Hosting and Scaling</h1>
            <p>Voice AI applications often have some traditional application components — web app frontends, API endpoints and other back-end elements. But the agent process itself is different enough from traditional app components that deploying and scaling voice AI comes with unique challenges.</p>

            <h2 id="hosting-architecture">10.1 Architecture</h2>

            <ul class="arrow-list">
              <li>The voice AI agent conversation loop is usually a long-running process (not a request/response function that exits when a single response is finished generating).</li>
              <li>Voice agents stream audio in realtime. Anything that stalls streaming can create audio glitches. (CPU spikes on a shared virtual machine, program flow that blocks audio thread execution even for as little as 10ms, etc.)</li>
              <li>Voice agents usually need either WebSocket or WebRTC connectivity. Cloud service network gateway and routing products don’t support WebSockets nearly as well as they support HTTP. They often don’t support UDP at all. (UDP is required for WebRTC.)</li>
            </ul>

            <p>For all of these reasons, it’s generally not possible to use a serverless framework like AWS Lambda or Google Cloud Run for voice AI.</p>
  
            <p>The best practice today for deploying voice AI agents is:</p>

            <ul class="arrow-list">
              <li>Once you’ve gotten past the prototyping phase, invest engineering time in creating lightweight tooling to build Docker (or similar) containers to deploy your agents.</li>
              <li>Push your container to your compute platform of choice. For simple deployments, you can just keep a fixed number of virtual machines running. At some point, though, you’ll want to hook into your platform’s tooling so you can autoscale, deploy new versions gracefully, implement good service discovery and failover, and build other at-scale devops requirements.</li>
              <li>Kubernetes is the standard these days for managing containers, deployments, and scaling. Kubernetes has a steep learning curve, but is supported on all of the major cloud platforms. Kubernetes has a very large ecosystem around it.</li>
              <li>For deploying software updates, you’ll want to set long drain times that allow existing connections to stay alive until sessions end. This is not terribly hard to do in Kubernetes, but the details depend on your k8s engine and version.</li>
              <li>Cold starts are a problem for voice AI agents, because fast connection times are important. Keeping an idle pool of agents is the easiest way to avoid long cold starts. If your workloads don’t require running large models locally, you can generally engineer fast container cold starts without too much effort.<sup>[59]</sup></li>
            </ul>

            <p>Virtual machine specs and container packing often trip people up when deploying to production for the first time. The specs your agents need will vary depending on what libraries you use and how much CPU-intensive work you do within your agent process. A good rule of thumb is to start by running a single agent per virtual machine CPU, with double the maximum amount of RAM you see an agent process consuming on your dev machines.<sup>[60]</sup></p>


        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[59] If you are running large models locally, advice about cold starts is well outside the scope of this guide. If you’re not already an expert on GPU and container optimization, you probably want to find an expert, rather than climb up that learning curve yourself (at least until you’re operating at a big enough scale to amortize the cost of developing the tooling you need).</p>
            </div>
            <div class="footnote">
              <p>[60] Make sure your container runtime is starting new agent processes on idle CPUs. This is not always the k8s default.</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="hosting-cost">10.2 Calculating per-minute cost</h2>

          <p>Voice AI cost varies widely depending on what models, APIs, and hosting infrastructure are used. Cost also depends on use case. For example, as discussed in <a href="#cost-comparison">cost comparison</a> above, per-minute costs are generally higher for longer sessions. And telephony is more expensive than WebRTC transport.</p> 

          <p>Costs range from $0.20 per minute or more if you are using a speech-to-speech API like the OpenAI Realtime API, to $0.10 per minute for batteries-included hosted agent platforms, down to $0.02 per minute for highly cost-optimized deployments running at significant scale.</p>

          <p>One mistake we sometimes see people make is cost-optimizing the agent hosting itself, before calculating the cost of the speech and LLM APIs. <strong>In general, cloud runtime costs for the agent processes themselves are less than one percent of total per-minute cost.</strong> It is almost never worth spending engineering effort on optimizing per-vCPU agent concurrency.</p>

          <p><a href="https://docs.google.com/spreadsheets/d/1-B3nv7fhwEoFmDs-phm280XK9phTep0qfyLKRNsEwVE/edit?gid=0#gid=0" target="_blank">Here is a spreadsheet</a> that you can copy and use as a starting point for calculating per-minute cost.</p>

          <div class="chapter-image">
            <img src="images/Figure 3000 General Costs.png" alt="A spreadsheet to calculate per-minute costs for voice AI agents" class="chunk-image-inline" width="80%">
            <p class="image-caption">A spreadsheet to calculate per-minute costs for voice AI agents</p>
          </div>

          <p>The numbers in the screenshot are for a self-hosted agent that uses Deepgram, GPT-4o, and Cartesia. For a ten-minute session, the per-minute cost is about two and a half cents. Transcription and the LLM inference are each about a quarter of the cost. Voice generation is about half of the cost. Hosting is less than one percent of the cost.</p>

          <p>Of course, this is not a realistic picture of what self-hosting actually costs. If you are building and maintaining all of your own hosting infrastructure, you'll need to set up, scale, and maintain a number of systems and capabilities in addition to the agents themselves.</p>

          <ul class="arrow-list">
            <li>Service discovery</li>
            <li>Load balancing</li>
            <li>Logging</li>
            <li>Monitoring</li>
            <li>Bandwidth</li>
            <li>Multiple regions</li>
            <li>Security</li>
            <li>Compliance and regulatory features (data residency, for example)</li>
            <li>Analytics</li>
            <li>Customer support</li>
          </ul>



        </div>

      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="future">11. What's coming in 2025</h1>
            
            <p>Speaking of the growth of AI engineering, voice AI grew enormously in 2024 and we expect this to continue in 2025.</p>
            
            <p></p>This expanding interest and adoption will create continuing progress in some important core areas:</p>

            <ul class="arrow-list">
              <li>More latency optimization from all model builders and service providers. For a long time, most people implementing services and almost all published benchmarks focused on throughput rather than latency. For voice AI, we care about time to first token much more than we care about tokens per second.</li>
              <li>Progress towards full integration of all the non-text modalities in models and APIs.</li>
              <li>More audio-specific features in testing and eval tools.</li>
              <li>Context caching APIs that support the needs of realtime multimodal use cases.</li>
              <li>New voice agent platforms from multiple providers.</li>
              <li>Speech-to-speech model APIs from multiple providers.</li>
              <li>Contextual speech models that are capable of incorporating context to improve transcription accuracy and voice generation quality.</li>
            </ul>

            <p>If you’re interested in hot takes about 2025 from four experts in the voice AI space, skip to <a href="https://www.youtube.com/live/B6zTwHh-abw?t=3065s" target="_blank">54:05 in the recording of the panel</a> from January’s San Francisco Voice AI Meetup. Karan Goel, Niamh Gavin, Shrestha Basu-Mallick, and Swyx all offered their predictions for what we’ll see in the coming year: universal memory, AI in Hollywood, moving from model imitating to model understanding, and a contrarian position on robotics.</p>
            
            <p>It’s going to be a fun year.</p>



        </div>
        
        <div class="chunk-notes">

          <div class="chunk-footnotes">
            <!-- No footnotes in this section -->
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="contributors">Contributors</h1>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="lead-author">Lead Author</h2>
            <p>Kwindla Hultman Kramer</p>
            <p>Thank you to Brooke Hopkins for help with the evals section, Zach Koch for insight into Llama performance and Ultravox, and Brendan Iribe for notes on the importance of the shift to contextual speech models. </p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="contributing-authors">Contributing Authors<sup>[61]</sup></h2>
            <p>aconchillo, markbackman, filipi87, Moishe, kwindla, kompfner, Vaibhav159, chadbailey59, jptaylor, vipyne, Allenmylath, TomTom101, adriancowham, imsakg, DominicStewart, marcus-daily, LewisWolfgang, mattieruth, golbin, adithyaxx, jamsea, vr000m, joachimchauvet, sahilsuman933, adnansiddiquei, sharvil, deshraj, balalofernandez, MaCaki, TheCodingLand, milo157, RJSkorski, nicougou, AngeloGiacco, kylegani, kunal-cai, lazeratops, EyrisCrafts, roey-priel, aashsach, jcbjoe, Dev-Khant, wg-daniel, cbrianhill, ankykong, nulyang, flixoflax, DANIIL0579, Antonyesk601, rahultayal22, lucasrothman, CarlKho-Minerva, 0xPatryk, pvilchez, pedro-a-n-moreira, RonakAgarwalVani, xtreme-sameer-vohra, shaiyon, soof-golan, yashn35, zboyles, balaji-atoa, eddieoz, mercuryyy, rahulunair, porcelaincode, weedge, wtlow003, zzz-heygen, adidoit, ArmanJR, Bnowako, chhao01, Regaddi, cyrilS-dev, DamienDeepgram, danthegoodman1, dleybz, ecdeng, gregschwartz, KevGTL, louisjoecodes, M1ngXU, mattmatters, MoofSoup, natestraub</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
            <h2 id="design">Design</h2>
            <p>Sascha Mombartz</p>
            <p>Akhil K G</p>

        </div>
        
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[61] GitHub usernames, <a href="https://github.com/pipecat-ai/pipecat/graphs/contributors" target="_blank">github.com/pipecat-ai/pipecat/graphs/contributors</a></p>
            </div>
          </div>
        </div>
      </div>

      <!-- More sections will be added as we process the content -->
    </main>

    <footer>
      <p>This book is available under the CC0 license. The authors have waived all their copyright and related rights in their works to the fullest extent allowed by law. You may use this work however you want and no attribution is required.</p>
      <p class="github-link">View and contribute to this guide on <a href="https://github.com/pipecat-ai/voice-ai-primer-web" target="_blank">GitHub</a>.</p>
    </footer>
  </div>

  <script src="script/footnotes.js"></script>
  
</body>
</html> 